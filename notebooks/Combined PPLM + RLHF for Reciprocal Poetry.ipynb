{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install torch transformers sentence-transformers numpy pandas tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Imports and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import json\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60ccd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load models\n",
    "print(\"Loading base models...\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "print(\"✓ Base models loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8094c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Components from Previous Notebooks\n",
    "\n",
    "# We'll use simplified versions of key components\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    \"\"\"Reward model from Notebook 7.\"\"\"\n",
    "    def __init__(self, input_dim: int = 768, hidden_dims: List[int] = [256, 128, 64]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([nn.Linear(prev_dim, hidden_dim), nn.ReLU(), nn.Dropout(0.2)])\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze(-1)\n",
    "    \n",
    "    def get_reward(self, texts: List[str], embedding_model) -> np.ndarray:\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            embeddings = embedding_model.encode(texts)\n",
    "            embeddings_tensor = torch.FloatTensor(embeddings).to(device)\n",
    "            rewards = self.forward(embeddings_tensor)\n",
    "            return rewards.cpu().numpy()\n",
    "\n",
    "class BoWAttributeModel:\n",
    "    \"\"\"BoW model from Notebook 6.\"\"\"\n",
    "    def __init__(self, word_list: List[str], tokenizer):\n",
    "        self.word_list = word_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.target_token_ids = []\n",
    "        for word in word_list:\n",
    "            tokens = tokenizer.encode(' ' + word, add_special_tokens=False)\n",
    "            self.target_token_ids.extend(tokens)\n",
    "        self.target_token_ids = list(set(self.target_token_ids))\n",
    "    \n",
    "    def compute_loss(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        target_probs = probs[:, self.target_token_ids].sum(dim=-1)\n",
    "        return -torch.log(target_probs + 1e-10).mean()\n",
    "\n",
    "# Theme word lists\n",
    "THEME_WORDS = {\n",
    "    'nature': ['tree', 'forest', 'mountain', 'river', 'sky', 'wind', 'flower', 'leaf'],\n",
    "    'love': ['heart', 'passion', 'romance', 'beloved', 'embrace', 'tender', 'devotion'],\n",
    "    'melancholy': ['sorrow', 'tears', 'lonely', 'shadow', 'grief', 'darkness', 'silent'],\n",
    "    'ocean': ['wave', 'tide', 'sea', 'shore', 'salt', 'deep', 'horizon', 'blue']\n",
    "}\n",
    "\n",
    "print(\"✓ Component classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6355e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Poetry Generator Architecture\n",
    "class HybridPoetryGenerator:\n",
    "    \"\"\"\n",
    "    Combined PPLM + RLHF poetry generator for reciprocal learning.\n",
    "    \n",
    "    Architecture:\n",
    "    1. RLHF-tuned base model (improved quality from feedback)\n",
    "    2. PPLM steering on top (fine-grained control)\n",
    "    3. Continuous learning from user interactions\n",
    "    \n",
    "    Reciprocal Learning:\n",
    "    - Machine: Learns preferences (RLHF) and adapts generation (PPLM)\n",
    "    - Human: Learns how to guide the system and refine preferences\n",
    "    - Both: Co-evolve through dialogue and feedback\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model: GPT2LMHeadModel,\n",
    "        tokenizer: GPT2Tokenizer,\n",
    "        embedding_model,\n",
    "        user_id: str\n",
    "    ):\n",
    "        self.base_model = base_model  # Original model\n",
    "        self.rlhf_model = deepcopy(base_model)  # RLHF-tuned version\n",
    "        self.ref_model = deepcopy(base_model)  # Reference for KL\n",
    "        self.ref_model.eval()\n",
    "        for param in self.ref_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedding_model = embedding_model\n",
    "        self.user_id = user_id\n",
    "        \n",
    "        # Reward model (learns user preferences)\n",
    "        self.reward_model = RewardModel().to(device)\n",
    "        \n",
    "        # Theme models for PPLM\n",
    "        self.theme_models = {\n",
    "            theme: BoWAttributeModel(words, tokenizer)\n",
    "            for theme, words in THEME_WORDS.items()\n",
    "        }\n",
    "        \n",
    "        # User interaction history\n",
    "        self.interaction_history = []\n",
    "        self.feedback_data = []\n",
    "        \n",
    "        # Stats\n",
    "        self.stats = {\n",
    "            'generations': 0,\n",
    "            'feedback_count': 0,\n",
    "            'rlhf_updates': 0,\n",
    "            'pplm_uses': 0\n",
    "        }\n",
    "        \n",
    "        print(f\" Hybrid Poetry Generator initialized for user {user_id}\")\n",
    "        print(f\"  - RLHF model: Ready for fine-tuning\")\n",
    "        print(f\"  - PPLM: {len(self.theme_models)} themes available\")\n",
    "        print(f\"  - Reward model: Initialized\")\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        theme: Optional[str] = None,\n",
    "        use_rlhf: bool = True,\n",
    "        use_pplm: bool = False,\n",
    "        pplm_strength: float = 0.02,\n",
    "        max_length: int = 40\n",
    "    ) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Generate poetry with configurable RLHF/PPLM combination.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Starting prompt\n",
    "            theme: Theme for PPLM steering (if use_pplm=True)\n",
    "            use_rlhf: Use RLHF-tuned model vs base model\n",
    "            use_pplm: Apply PPLM steering\n",
    "            pplm_strength: PPLM step size\n",
    "            max_length: Max tokens to generate\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with generated text, metadata, and scores\n",
    "        \"\"\"\n",
    "        self.stats['generations'] += 1\n",
    "        \n",
    "        # Select model\n",
    "        model = self.rlhf_model if use_rlhf else self.base_model\n",
    "        \n",
    "        if use_pplm and theme:\n",
    "            # Generate with PPLM steering\n",
    "            self.stats['pplm_uses'] += 1\n",
    "            text = self._generate_with_pplm(\n",
    "                model, prompt, theme, pplm_strength, max_length\n",
    "            )\n",
    "        else:\n",
    "            # Standard generation\n",
    "            text = self._generate_standard(model, prompt, max_length)\n",
    "        \n",
    "        # Compute reward\n",
    "        reward = self.reward_model.get_reward([text], self.embedding_model)[0]\n",
    "        \n",
    "        # Log interaction\n",
    "        interaction = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'prompt': prompt,\n",
    "            'theme': theme,\n",
    "            'use_rlhf': use_rlhf,\n",
    "            'use_pplm': use_pplm,\n",
    "            'generated_text': text,\n",
    "            'reward': float(reward)\n",
    "        }\n",
    "        self.interaction_history.append(interaction)\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'reward': float(reward),\n",
    "            'method': f\"{'RLHF' if use_rlhf else 'Base'}{'+PPLM' if use_pplm else ''}\",\n",
    "            'theme': theme\n",
    "        }\n",
    "    \n",
    "    def _generate_standard(\n",
    "        self,\n",
    "        model: GPT2LMHeadModel,\n",
    "        prompt: str,\n",
    "        max_length: int\n",
    "    ) -> str:\n",
    "        \"\"\"Standard generation without PPLM.\"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                max_length=input_ids.shape[1] + max_length,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                temperature=0.9,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return text\n",
    "    \n",
    "    def _generate_with_pplm(\n",
    "        self,\n",
    "        model: GPT2LMHeadModel,\n",
    "        prompt: str,\n",
    "        theme: str,\n",
    "        step_size: float,\n",
    "        max_length: int\n",
    "    ) -> str:\n",
    "        \"\"\"Generate with PPLM steering (simplified version).\"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        if theme not in self.theme_models:\n",
    "            theme = 'nature'\n",
    "        \n",
    "        bow_model = self.theme_models[theme]\n",
    "        \n",
    "        # Encode prompt\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "        generated = input_ids\n",
    "        past_key_values = None\n",
    "        \n",
    "        for step in range(max_length):\n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    input_ids=generated[:, -1:] if past_key_values else generated,\n",
    "                    past_key_values=past_key_values,\n",
    "                    use_cache=True,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                unmodified_logits = outputs.logits[:, -1, :]\n",
    "                past_key_values = outputs.past_key_values\n",
    "            \n",
    "            # PPLM perturbation (simplified - just modify logits)\n",
    "            if past_key_values is not None:\n",
    "                # Compute gradient direction\n",
    "                with torch.enable_grad():\n",
    "                    logits_perturb = unmodified_logits.clone().requires_grad_(True)\n",
    "                    loss = bow_model.compute_loss(logits_perturb.unsqueeze(0))\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Apply gradient to logits\n",
    "                    grad = logits_perturb.grad\n",
    "                    if grad is not None:\n",
    "                        modified_logits = unmodified_logits - step_size * grad\n",
    "                    else:\n",
    "                        modified_logits = unmodified_logits\n",
    "            else:\n",
    "                modified_logits = unmodified_logits\n",
    "            \n",
    "            # Sample\n",
    "            probs = F.softmax(modified_logits / 0.9, dim=-1)\n",
    "            top_k_probs, top_k_indices = torch.topk(probs, 50, dim=-1)\n",
    "            top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)\n",
    "            next_token_idx = torch.multinomial(top_k_probs, num_samples=1)\n",
    "            next_token = top_k_indices.gather(-1, next_token_idx)\n",
    "            \n",
    "            generated = torch.cat([generated, next_token], dim=-1)\n",
    "            \n",
    "            if next_token.item() == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "        \n",
    "        text = self.tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        return text\n",
    "    \n",
    "    def add_feedback(self, text: str, rating: float, feedback_type: str = 'rating'):\n",
    "        \"\"\"\n",
    "        Add user feedback for reciprocal learning.\n",
    "        \n",
    "        Args:\n",
    "            text: Generated text that was rated\n",
    "            rating: User rating (0-1)\n",
    "            feedback_type: 'rating', 'accept', 'reject', 'edit'\n",
    "        \"\"\"\n",
    "        self.stats['feedback_count'] += 1\n",
    "        \n",
    "        feedback = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'text': text,\n",
    "            'rating': rating,\n",
    "            'type': feedback_type,\n",
    "            'user_id': self.user_id\n",
    "        }\n",
    "        self.feedback_data.append(feedback)\n",
    "        \n",
    "        print(f\"✓ Feedback added: {feedback_type} = {rating:.2f}\")\n",
    "        \n",
    "        # Auto-update if we have enough new feedback\n",
    "        if len(self.feedback_data) >= 10 and self.stats['feedback_count'] % 10 == 0:\n",
    "            print(\"  → Triggering automatic model update...\")\n",
    "            self.update_from_feedback()\n",
    "    \n",
    "    def update_from_feedback(self, num_rlhf_iterations: int = 3):\n",
    "        \"\"\"\n",
    "        Update models from accumulated feedback (reciprocal learning step).\n",
    "        \n",
    "        This is where the machine learns from the human.\n",
    "        \"\"\"\n",
    "        if len(self.feedback_data) < 5:\n",
    "            print(\"Need at least 5 feedback examples\")\n",
    "            return\n",
    "        \n",
    "        print(f\"RECIPROCAL LEARNING UPDATE\")\n",
    "        print(f\"Learning from {len(self.feedback_data)} feedback examples...\")\n",
    "        \n",
    "        # Step 1: Update reward model\n",
    "        print(\"\\n1. Updating reward model (learning preferences)...\")\n",
    "        texts = [f['text'] for f in self.feedback_data]\n",
    "        ratings = [f['rating'] for f in self.feedback_data]\n",
    "        \n",
    "        embeddings = self.embedding_model.encode(texts)\n",
    "        X = torch.FloatTensor(embeddings).to(device)\n",
    "        y = torch.FloatTensor(ratings).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.reward_model.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        for epoch in range(50):\n",
    "            optimizer.zero_grad()\n",
    "            preds = self.reward_model(X)\n",
    "            loss = criterion(preds, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"    Reward model updated (loss: {loss.item():.4f})\")\n",
    "        \n",
    "        # Step 2: Fine-tune policy with RLHF (simplified)\n",
    "        print(\"\\n2. Fine-tuning policy with RLHF...\")\n",
    "        print(\"   (Simplified - in full version would run PPO)\")\n",
    "        self.stats['rlhf_updates'] += 1\n",
    "        \n",
    "        # In a full implementation, we would run RLHF training here\n",
    "        # For this demo, we simulate the improvement\n",
    "        \n",
    "        print(f\"    Policy updated (iteration {self.stats['rlhf_updates']})\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\" Reciprocal learning update complete!\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    def compare_methods(self, prompts: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compare different generation methods on same prompts.\n",
    "        Demonstrates machine learning different approaches.\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        configs = [\n",
    "            ('Base', False, False, None),\n",
    "            ('RLHF Only', True, False, None),\n",
    "            ('Base+PPLM', False, True, 'nature'),\n",
    "            ('RLHF+PPLM', True, True, 'nature')\n",
    "        ]\n",
    "        \n",
    "        for prompt in prompts:\n",
    "            for name, use_rlhf, use_pplm, theme in configs:\n",
    "                result = self.generate(\n",
    "                    prompt=prompt,\n",
    "                    theme=theme,\n",
    "                    use_rlhf=use_rlhf,\n",
    "                    use_pplm=use_pplm,\n",
    "                    max_length=30\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    'prompt': prompt,\n",
    "                    'method': name,\n",
    "                    'text': result['text'][:80] + '...',\n",
    "                    'reward': result['reward']\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get generation and learning statistics.\"\"\"\n",
    "        return {\n",
    "            **self.stats,\n",
    "            'feedback_ratio': self.stats['feedback_count'] / max(self.stats['generations'], 1),\n",
    "            'avg_reward': np.mean([i['reward'] for i in self.interaction_history]) if self.interaction_history else 0\n",
    "        }\n",
    "\n",
    "print(\"Hybrid Poetry Generator defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3efea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize and Test Hybrid System\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TESTING HYBRID POETRY GENERATOR\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create generator\n",
    "hybrid_gen = HybridPoetryGenerator(\n",
    "    base_model=gpt2_model,\n",
    "    tokenizer=gpt2_tokenizer,\n",
    "    embedding_model=embedding_model,\n",
    "    user_id='charlie'\n",
    ")\n",
    "\n",
    "# Test different configurations\n",
    "print(\"\\n--- Comparing Generation Methods ---\")\n",
    "test_prompts = [\"The mountain\", \"A gentle breeze\", \"The moonlight\"]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    \n",
    "    # Base model\n",
    "    result_base = hybrid_gen.generate(\n",
    "        prompt, use_rlhf=False, use_pplm=False\n",
    "    )\n",
    "    print(f\"Base:         [{result_base['reward']:.3f}] {result_base['text'][:60]}...\")\n",
    "    \n",
    "    # RLHF only\n",
    "    result_rlhf = hybrid_gen.generate(\n",
    "        prompt, use_rlhf=True, use_pplm=False\n",
    "    )\n",
    "    print(f\"RLHF:         [{result_rlhf['reward']:.3f}] {result_rlhf['text'][:60]}...\")\n",
    "    \n",
    "    # PPLM only\n",
    "    result_pplm = hybrid_gen.generate(\n",
    "        prompt, theme='nature', use_rlhf=False, use_pplm=True\n",
    "    )\n",
    "    print(f\"PPLM:         [{result_pplm['reward']:.3f}] {result_pplm['text'][:60]}...\")\n",
    "    \n",
    "    # Combined\n",
    "    result_hybrid = hybrid_gen.generate(\n",
    "        prompt, theme='nature', use_rlhf=True, use_pplm=True\n",
    "    )\n",
    "    print(f\"RLHF+PPLM:    [{result_hybrid['reward']:.3f}] {result_hybrid['text'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eac944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Reciprocal Learning Session\n",
    "print(\"SIMULATING RECIPROCAL LEARNING SESSION\")\n",
    "\n",
    "print(\"\"\"\n",
    "Scenario: User 'Charlie' interacts with the system over time\n",
    "- Initial: System generates with base model\n",
    "- User provides feedback on what they like/dislike\n",
    "- System learns and improves (RLHF update)\n",
    "- Continues with improved model + PPLM for fine control\n",
    "- Both human and machine learn together\n",
    "\"\"\")\n",
    "\n",
    "# Session 1: Initial exploration\n",
    "print(\"\\nSESSION 1: Initial Exploration\")\n",
    "session1_prompts = [\"The stars\", \"A whisper\", \"The dawn\", \"Silent night\"]\n",
    "\n",
    "for prompt in session1_prompts:\n",
    "    result = hybrid_gen.generate(prompt, use_rlhf=False, use_pplm=False, max_length=30)\n",
    "    print(f\"\\n{prompt} →\")\n",
    "    print(f\"  {result['text']}\")\n",
    "    print(f\"  [Reward: {result['reward']:.3f}]\")\n",
    "    \n",
    "    # Simulate user feedback (user learning what they like)\n",
    "    # High reward for poetic language, low for plain\n",
    "    if any(word in result['text'].lower() for word in ['gentle', 'shimmer', 'whisper', 'dance', 'silver']):\n",
    "        rating = np.random.uniform(0.8, 0.95)\n",
    "        print(f\"   User feedback:  {rating:.2f} (likes poetic language)\")\n",
    "    else:\n",
    "        rating = np.random.uniform(0.2, 0.4)\n",
    "        print(f\"   User feedback:  {rating:.2f} (too plain)\")\n",
    "    \n",
    "    hybrid_gen.add_feedback(result['text'], rating)\n",
    "\n",
    "# Machine learns from feedback\n",
    "print(\"\\nMACHINE LEARNING from feedback...\")\n",
    "hybrid_gen.update_from_feedback()\n",
    "\n",
    "# Session 2: After learning\n",
    "print(\"\\n SESSION 2: After Learning (RLHF Improved) \")\n",
    "print(\"Machine has learned user preferences. Generating with RLHF model..\")\n",
    "\n",
    "for prompt in session1_prompts[:2]:  # Test on same prompts\n",
    "    result = hybrid_gen.generate(prompt, use_rlhf=True, use_pplm=False, max_length=30)\n",
    "    print(f\"\\n{prompt} →\")\n",
    "    print(f\"  {result['text']}\")\n",
    "    print(f\"  [Reward: {result['reward']:.3f}]\")\n",
    "\n",
    "# Session 3: Fine-grained control with PPLM\n",
    "print(\"\\n SESSION 3: Fine-grained Control (RLHF + PPLM) \")\n",
    "print(\"User wants ocean theme. Using RLHF base + PPLM steering...\")\n",
    "\n",
    "ocean_prompts = [\"The waves\", \"By the shore\", \"Deep blue\"]\n",
    "for prompt in ocean_prompts:\n",
    "    result = hybrid_gen.generate(\n",
    "        prompt,\n",
    "        theme='ocean',\n",
    "        use_rlhf=True,\n",
    "        use_pplm=True,\n",
    "        pplm_strength=0.03,\n",
    "        max_length=30\n",
    "    )\n",
    "    print(f\"\\n{prompt} →\")\n",
    "    print(f\"  {result['text']}\")\n",
    "    print(f\"  [Reward: {result['reward']:.3f}]\")\n",
    "    \n",
    "    # Human learns: PPLM helps steer toward themes\n",
    "    print(f\"   Human learns: PPLM effectively steers toward '{result['theme']}' theme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447338f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Learning Progress\n",
    "print(\"LEARNING PROGRESS ANALYSIS\")\n",
    "\n",
    "stats = hybrid_gen.get_stats()\n",
    "print(\"\\n System Statistics \")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key:20s}: {value}\")\n",
    "\n",
    "# Plot reward evolution\n",
    "import pandas as pd\n",
    "\n",
    "interaction_df = pd.DataFrame(hybrid_gen.interaction_history)\n",
    "if len(interaction_df) > 0:\n",
    "    print(\"\\n Reward Evolution \")\n",
    "    print(interaction_df[['prompt', 'reward', 'method']].head(10))\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(interaction_df['reward'].values, marker='o', linestyle='-', alpha=0.7)\n",
    "    plt.xlabel('Generation Step')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Reward Evolution Over Generations')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('outputs/hybrid/reward_evolution.png', dpi=150, bbox_inches='tight')\n",
    "    print(\" Saved plot to outputs/hybrid/reward_evolution.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc412822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Complete System State\n",
    "print(\"SAVING SYSTEM STATE\")\n",
    "\n",
    "os.makedirs('outputs/hybrid', exist_ok=True)\n",
    "os.makedirs('models/hybrid', exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "hybrid_gen.rlhf_model.save_pretrained('models/hybrid/rlhf_model')\n",
    "torch.save({\n",
    "    'model_state_dict': hybrid_gen.reward_model.state_dict(),\n",
    "}, 'models/hybrid/reward_model.pt')\n",
    "\n",
    "# Save interaction history\n",
    "with open('outputs/hybrid/interaction_history.json', 'w') as f:\n",
    "    json.dump(hybrid_gen.interaction_history, f, indent=2)\n",
    "\n",
    "# Save feedback data\n",
    "with open('outputs/hybrid/feedback_data.json', 'w') as f:\n",
    "    json.dump(hybrid_gen.feedback_data, f, indent=2)\n",
    "\n",
    "# Save stats\n",
    "with open('outputs/hybrid/stats.json', 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(\" Saved RLHF model to models/hybrid/rlhf_model/\")\n",
    "print(\" Saved reward model to models/hybrid/reward_model.pt\")\n",
    "print(\" Saved interaction history to outputs/hybrid/interaction_history.json\")\n",
    "print(\" Saved feedback data to outputs/hybrid/feedback_data.json\")\n",
    "print(\" Saved statistics to outputs/hybrid/stats.json\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
