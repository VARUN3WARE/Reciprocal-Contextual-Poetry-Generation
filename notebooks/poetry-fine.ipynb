{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13705362,"sourceType":"datasetVersion","datasetId":8718464}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    TrainingArguments, \n    Trainer,\n    EarlyStoppingCallback\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n\n# Model selection (use smaller model for Kaggle)\nmodel_name = \"gpt2-large\"  # Better than base, fits in P100\n# For better results (if you have GPU memory): \"mistralai/Mistral-7B-v0.1\"\n\nprint(f\"Loading {model_name}...\")\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Load dataset\ndataset = load_dataset(\"json\", data_files=\"/kaggle/working/poems.jsonl\")\ndataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n\nprint(f\"Train: {len(dataset['train'])} | Val: {len(dataset['test'])}\")\n\n# Tokenization function\ndef tokenize(examples):\n    # Combine prompt and completion with proper formatting\n    texts = [\n        f\"{prompt}{completion}\"\n        for prompt, completion in zip(examples[\"prompt\"], examples[\"completion\"])\n    ]\n    \n    encodings = tokenizer(\n        texts,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=512,\n        return_tensors=None\n    )\n    \n    encodings[\"labels\"] = encodings[\"input_ids\"].copy()\n    return encodings\n\n# Tokenize dataset\ntokenized_dataset = dataset.map(\n    tokenize,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names\n)\n\nprint(\"Dataset tokenized\")\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# LoRA configuration\nlora_config = LoraConfig(\n    r=16,                        # Rank\n    lora_alpha=32,               # Scaling factor\n    target_modules=[             # Target attention layers\n        \"c_attn\",                # For GPT-2\n        # For Mistral use: \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"\n    ],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Apply LoRA\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./poetry-lora-model\",\n    num_train_epochs=5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    warmup_steps=100,\n    fp16=True,\n    logging_steps=50,\n    eval_strategy=\"steps\",\n    eval_steps=200,\n    save_strategy=\"steps\",\n    save_steps=200,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\",\n    seed=42,\n    dataloader_num_workers=0,\n    remove_unused_columns=False\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    callbacks=[\n        EarlyStoppingCallback(\n            early_stopping_patience=3,\n            early_stopping_threshold=0.01\n        )\n    ]\n)\n\nprint(\"\\nStarting LoRA training...\")\ntrainer.train()\n\n# Save model\noutput_dir = \"./poetry-lora-final\"\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(f\"\\nLoRA model saved to {output_dir}\")\n\n# Test generation\nprint(\"\\nTesting poetry generation:\")\nmodel.eval()\n\ntest_prompts = [\n    \"Write a poem about love\",\n    \"Write a poem starting with: The moon\",\n    \"Write a creative poem\"\n]\n\nfor prompt in test_prompts:\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.85,\n            top_k=50,\n            top_p=0.9,\n            do_sample=True,\n            repetition_penalty=1.2,\n            no_repeat_ngram_size=3,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"\\n{'-'*60}\")\n    print(f\"Prompt: {prompt}\")\n    print(f\"\\n{generated}\")\n    print(f\"{'-'*60}\")\n\nprint(\"\\nTraining complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T01:08:15.786761Z","iopub.execute_input":"2025-11-13T01:08:15.787027Z","iopub.status.idle":"2025-11-13T02:44:41.906936Z","shell.execute_reply.started":"2025-11-13T01:08:15.787007Z","shell.execute_reply":"2025-11-13T02:44:41.906100Z"}},"outputs":[{"name":"stderr","text":"2025-11-13 01:08:36.259036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762996116.502499      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762996116.571623      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"GPU: Tesla P100-PCIE-16GB\nLoading gpt2-large...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50c02452e9f64b7f90b12ef4cc219be9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2895d0d08e5f44dd9a3df3fb2186b3a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d92659ccab1e4ae7857b180a1f55a0f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"823e9129b24a45e281ed38cebb39bd60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c91e6b9e707c4f638c340ded66e34bc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b027a2cc0ba4142aa4a40a867a0826c"}},"metadata":{}},{"name":"stdout","text":"Train: 3931 | Val: 437\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3931 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"224c336c80be46a5b9f738ee961feca9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/437 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d90ca3a838d04bd49afeda99fa1fff55"}},"metadata":{}},{"name":"stdout","text":"Dataset tokenized\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70595f9516f84829b9efc9a34912ac74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c705150b90e47e5890c2e2dcb1fc7b5"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 2,949,120 || all params: 776,979,200 || trainable%: 0.3796\n\nStarting LoRA training...\n","output_type":"stream"},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1230 1:35:07 < 21:55, 0.17 it/s, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>1.483200</td>\n      <td>1.417541</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.414900</td>\n      <td>1.395075</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.435500</td>\n      <td>1.386361</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.438000</td>\n      <td>1.381945</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.470600</td>\n      <td>1.379042</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nLoRA model saved to ./poetry-lora-final\n\nTesting poetry generation:\n\n------------------------------------------------------------\nPrompt: Write a poem about love\n\nWrite a poem about love and the\n\nLove and the sun\nWill stay forever as you see them together in this day,\nYou will always be together like brothers and sisters and lovers for ever.\n------------------------------------------------------------\n\n------------------------------------------------------------\nPrompt: Write a poem starting with: The moon\n\nWrite a poem starting with: The moon and\n\nThe moon and the stars are always together\nAnd\nI am on earth.\nIt is dark so dark and there are clouds everywhere,\nBut it is beautiful for me to see the bright colors of day and night;\nIn the daytime all my thoughts focus around these two;\nTo hear the birds sing and to look at the sunset from my window;\nIt was also a beautiful day to be in the garden.\n------------------------------------------------------------\n\n------------------------------------------------------------\nPrompt: Write a creative poem\n\nWrite a creative poem\n\nSometime when,\nI was in the middle of my work,\nA man came up to me and said: \"Oh! dear sir,\nIf you are doing well as an artist, you should spend some time in your garden,\nAnd cultivate it for all its beauty and fruitfulness.\"\n------------------------------------------------------------\n\nTraining complete!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Loading LoRA model...\")\n\n# Load base model and tokenizer\nbase_model_name = \"gpt2-large\"  # Must match training\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/poetry-lora-final\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Load LoRA weights\nmodel = PeftModel.from_pretrained(model, \"/kaggle/working/poetry-lora-final\")\nmodel.eval()\n\nprint(f\"Model loaded on {model.device}\\n\")\n\ndef generate_poem(prompt, max_length=200, temperature=0.85):\n    \"\"\"Generate a poem based on prompt\"\"\"\n    \n    # Format prompt if needed\n    if not prompt.startswith(\"Write\"):\n        prompt = f\"Write a poem about {prompt}\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_length,\n            temperature=temperature,\n            top_k=50,\n            top_p=0.92,\n            do_sample=True,\n            repetition_penalty=1.25,\n            no_repeat_ngram_size=3,\n            pad_token_id=tokenizer.eos_token_id,\n            eos_token_id=tokenizer.eos_token_id\n        )\n    \n    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract only the poem part (remove prompt)\n    if prompt in generated:\n        poem = generated.split(prompt, 1)[1].strip()\n    else:\n        poem = generated\n    \n    return poem\n\n# Test examples\nprint(\"Sample Poems:\\n\")\n\ntest_themes = [\n    \"love and romance\",\n    \"the moonlight\",\n    \"nature and gardens\",\n    \"time passing\",\n    \"friendship\"]]\n\nfor theme in test_themes:\n    print(f\"Theme: {theme}\")\n    poem = generate_poem(theme)\n    print(poem)\n    print(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# Interactive mode\nprint(\"Interactive Poetry Generator\")\nprint(\"Enter themes or prompts (or 'quit' to exit)\\n\")\n\nwhile True:\n    user_input = input(\"Theme/Prompt: \").strip()\n    \n    if user_input.lower() in ['quit', 'exit', 'q']:\n        print(\"Goodbye!\")\n        break\n    \n    if not user_input:\n        continue\n    \n    print(\"\\nGenerating...\\n\")\n    poem = generate_poem(user_input)\n    print(poem)\n    print(\"\\n\" + \"-\"*60 + \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}