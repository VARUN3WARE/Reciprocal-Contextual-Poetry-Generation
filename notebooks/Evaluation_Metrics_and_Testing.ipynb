{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rb3TqZYTfpVn",
        "outputId": "a5c828be-f996-4301-e870-aecadac61988"
      },
      "outputs": [],
      "source": [
        "!pip install numpy pandas rouge-score nltk matplotlib seaborn\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Tuple, Set, Optional\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Download NLTK data (run once)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "print(\"✓ All imports successful\")\n",
        "\n",
        "\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    \"\"\"Simple tokenization.\"\"\"\n",
        "    # Convert to lowercase and extract words\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return tokens\n",
        "\n",
        "def compute_distinct_n(texts: List[str], n: int = 1) -> float:\n",
        "    \"\"\"\n",
        "    Compute Distinct-N metric for a set of texts.\n",
        "\n",
        "    Distinct-N measures lexical diversity:\n",
        "    distinct_n = (# unique n-grams) / (# total n-grams)\n",
        "\n",
        "    Args:\n",
        "        texts: List of generated texts\n",
        "        n: N-gram size (1 for unigrams, 2 for bigrams, etc.)\n",
        "\n",
        "    Returns:\n",
        "        Distinct-N score (0-1, higher is more diverse)\n",
        "    \"\"\"\n",
        "    all_ngrams = []\n",
        "\n",
        "    for text in texts:\n",
        "        tokens = tokenize(text)\n",
        "\n",
        "        # Generate n-grams\n",
        "        ngrams = [\n",
        "            tuple(tokens[i:i+n])\n",
        "            for i in range(len(tokens) - n + 1)\n",
        "        ]\n",
        "        all_ngrams.extend(ngrams)\n",
        "\n",
        "    if not all_ngrams:\n",
        "        return 0.0\n",
        "\n",
        "    unique_ngrams = len(set(all_ngrams))\n",
        "    total_ngrams = len(all_ngrams)\n",
        "\n",
        "    return unique_ngrams / total_ngrams\n",
        "\n",
        "def compute_all_distinct_metrics(texts: List[str]) -> Dict[str, float]:\n",
        "    \"\"\"Compute Distinct-1, Distinct-2, and Distinct-3.\"\"\"\n",
        "    return {\n",
        "        'distinct_1': compute_distinct_n(texts, n=1),\n",
        "        'distinct_2': compute_distinct_n(texts, n=2),\n",
        "        'distinct_3': compute_distinct_n(texts, n=3)\n",
        "    }\n",
        "\n",
        "# Test distinct-N\n",
        "print(\"\\n--- Testing Distinct-N Metrics ---\")\n",
        "\n",
        "test_texts_diverse = [\n",
        "    \"The mountain stands tall against the sky\",\n",
        "    \"Rivers flow endlessly toward the sea\",\n",
        "    \"Ancient forests whisper with the wind\",\n",
        "    \"Stars shine brightly in the night\",\n",
        "    \"Flowers bloom across the meadow\"\n",
        "]\n",
        "\n",
        "test_texts_repetitive = [\n",
        "    \"The mountain is tall\",\n",
        "    \"The mountain is very tall\",\n",
        "    \"The tall mountain stands\",\n",
        "    \"Mountains are tall\",\n",
        "    \"The mountain is really tall\"\n",
        "]\n",
        "\n",
        "print(\"\\nDiverse texts:\")\n",
        "diverse_metrics = compute_all_distinct_metrics(test_texts_diverse)\n",
        "for key, value in diverse_metrics.items():\n",
        "    print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nRepetitive texts:\")\n",
        "repetitive_metrics = compute_all_distinct_metrics(test_texts_repetitive)\n",
        "for key, value in repetitive_metrics.items():\n",
        "    print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "\n",
        "def compute_rouge_l(candidates: List[str], reference: str) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Compute ROUGE-L scores for candidates against a reference.\n",
        "\n",
        "    Args:\n",
        "        candidates: List of generated texts\n",
        "        reference: Reference text\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with mean ROUGE-L scores\n",
        "    \"\"\"\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "    scores = []\n",
        "    for candidate in candidates:\n",
        "        score = scorer.score(reference, candidate)\n",
        "        scores.append({\n",
        "            'precision': score['rougeL'].precision,\n",
        "            'recall': score['rougeL'].recall,\n",
        "            'fmeasure': score['rougeL'].fmeasure\n",
        "        })\n",
        "\n",
        "    # Compute means\n",
        "    mean_scores = {\n",
        "        'rouge_l_precision': np.mean([s['precision'] for s in scores]),\n",
        "        'rouge_l_recall': np.mean([s['recall'] for s in scores]),\n",
        "        'rouge_l_fmeasure': np.mean([s['fmeasure'] for s in scores])\n",
        "    }\n",
        "\n",
        "    return mean_scores, scores\n",
        "\n",
        "# Test ROUGE-L\n",
        "print(\"\\n--- Testing ROUGE-L Metric ---\")\n",
        "\n",
        "reference_text = \"The majestic mountain stands tall against the clear blue sky\"\n",
        "candidate_texts = [\n",
        "    \"The mountain stands tall against the sky\",\n",
        "    \"Mountains rise high under blue skies\",\n",
        "    \"The peak towers over the valley below\",\n",
        "    \"Tall mountains reach toward the heavens\"\n",
        "]\n",
        "\n",
        "rouge_mean, rouge_individual = compute_rouge_l(candidate_texts, reference_text)\n",
        "\n",
        "print(f\"\\nReference: {reference_text}\")\n",
        "print(\"\\nMean ROUGE-L scores:\")\n",
        "for key, value in rouge_mean.items():\n",
        "    print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nIndividual scores:\")\n",
        "for i, (cand, score) in enumerate(zip(candidate_texts, rouge_individual), 1):\n",
        "    print(f\"\\n  [{i}] {cand}\")\n",
        "    print(f\"      F-measure: {score['fmeasure']:.4f}\")\n",
        "\n",
        "\n",
        "def compute_self_bleu(texts: List[str], n: int = 4) -> float:\n",
        "    \"\"\"\n",
        "    Compute Self-BLEU to measure diversity.\n",
        "\n",
        "    Lower Self-BLEU indicates higher diversity (texts are less similar to each other).\n",
        "\n",
        "    Args:\n",
        "        texts: List of texts\n",
        "        n: Maximum n-gram order\n",
        "\n",
        "    Returns:\n",
        "        Mean Self-BLEU score\n",
        "    \"\"\"\n",
        "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    scores = []\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        # Use all other texts as references\n",
        "        references = [tokenize(t) for j, t in enumerate(texts) if j != i]\n",
        "        if not references:\n",
        "            continue\n",
        "\n",
        "        candidate = tokenize(text)\n",
        "\n",
        "        # Compute BLEU with smoothing\n",
        "        try:\n",
        "            score = sentence_bleu(\n",
        "                references,\n",
        "                candidate,\n",
        "                weights=[1/n] * n,\n",
        "                smoothing_function=smoothing\n",
        "            )\n",
        "            scores.append(score)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return np.mean(scores) if scores else 0.0\n",
        "\n",
        "# Test Self-BLEU\n",
        "print(\"\\n--- Testing Self-BLEU Metric ---\")\n",
        "\n",
        "print(\"\\nDiverse texts:\")\n",
        "self_bleu_diverse = compute_self_bleu(test_texts_diverse)\n",
        "print(f\"  Self-BLEU: {self_bleu_diverse:.4f} (lower is more diverse)\")\n",
        "\n",
        "print(\"\\nRepetitive texts:\")\n",
        "self_bleu_repetitive = compute_self_bleu(test_texts_repetitive)\n",
        "print(f\"  Self-BLEU: {self_bleu_repetitive:.4f} (lower is more diverse)\")\n",
        "\n",
        "def compute_length_stats(texts: List[str]) -> Dict[str, float]:\n",
        "    \"\"\"Compute length statistics for texts.\"\"\"\n",
        "\n",
        "    char_lengths = [len(t) for t in texts]\n",
        "    word_lengths = [len(tokenize(t)) for t in texts]\n",
        "\n",
        "    return {\n",
        "        'mean_chars': np.mean(char_lengths),\n",
        "        'std_chars': np.std(char_lengths),\n",
        "        'mean_words': np.mean(word_lengths),\n",
        "        'std_words': np.std(word_lengths),\n",
        "        'min_words': min(word_lengths),\n",
        "        'max_words': max(word_lengths)\n",
        "    }\n",
        "\n",
        "# Test length stats\n",
        "print(\"\\n--- Testing Length Statistics ---\")\n",
        "length_stats = compute_length_stats(test_texts_diverse)\n",
        "for key, value in length_stats.items():\n",
        "    print(f\"  {key}: {value:.2f}\")\n",
        "\n",
        "def compute_vocabulary_metrics(texts: List[str]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Compute vocabulary richness metrics.\n",
        "\n",
        "    Includes:\n",
        "    - Type-Token Ratio (TTR): unique words / total words\n",
        "    - Lexical density: content words / total words\n",
        "    \"\"\"\n",
        "    all_tokens = []\n",
        "    for text in texts:\n",
        "        all_tokens.extend(tokenize(text))\n",
        "\n",
        "    if not all_tokens:\n",
        "        return {\n",
        "            'vocabulary_size': 0,\n",
        "            'total_tokens': 0,\n",
        "            'type_token_ratio': 0.0\n",
        "        }\n",
        "\n",
        "    vocab_size = len(set(all_tokens))\n",
        "    total_tokens = len(all_tokens)\n",
        "    ttr = vocab_size / total_tokens if total_tokens > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'vocabulary_size': vocab_size,\n",
        "        'total_tokens': total_tokens,\n",
        "        'type_token_ratio': ttr\n",
        "    }\n",
        "\n",
        "# Test vocabulary metrics\n",
        "print(\"\\n--- Testing Vocabulary Metrics ---\")\n",
        "vocab_metrics = compute_vocabulary_metrics(test_texts_diverse)\n",
        "for key, value in vocab_metrics.items():\n",
        "    print(f\"  {key}: {value if isinstance(value, int) else f'{value:.4f}'}\")\n",
        "\n",
        "\n",
        "def evaluate_candidates(\n",
        "    candidates: List[str],\n",
        "    reference: Optional[str] = None,\n",
        "    compute_rouge: bool = True\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of candidate texts.\n",
        "\n",
        "    Args:\n",
        "        candidates: List of generated texts\n",
        "        reference: Optional reference text for ROUGE\n",
        "        compute_rouge: Whether to compute ROUGE (requires reference)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of all metrics\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    # Distinct-N\n",
        "    distinct_metrics = compute_all_distinct_metrics(candidates)\n",
        "    metrics.update(distinct_metrics)\n",
        "\n",
        "    # Self-BLEU\n",
        "    if len(candidates) > 1:\n",
        "        metrics['self_bleu'] = compute_self_bleu(candidates)\n",
        "\n",
        "    # Length statistics\n",
        "    length_stats = compute_length_stats(candidates)\n",
        "    metrics.update(length_stats)\n",
        "\n",
        "    # Vocabulary metrics\n",
        "    vocab_metrics = compute_vocabulary_metrics(candidates)\n",
        "    metrics.update(vocab_metrics)\n",
        "\n",
        "    # ROUGE (if reference provided)\n",
        "    if reference and compute_rouge:\n",
        "        rouge_mean, _ = compute_rouge_l(candidates, reference)\n",
        "        metrics.update(rouge_mean)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Test comprehensive evaluation\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COMPREHENSIVE EVALUATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "all_metrics = evaluate_candidates(\n",
        "    test_texts_diverse,\n",
        "    reference=\"The mountain landscape stretches far across the horizon\",\n",
        "    compute_rouge=True\n",
        ")\n",
        "\n",
        "print(\"\\n--- All Metrics ---\")\n",
        "for key, value in sorted(all_metrics.items()):\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {key:25s}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"  {key:25s}: {value}\")\n",
        "\n",
        "\n",
        "def load_and_evaluate_generation_file(filepath: str) -> Dict:\n",
        "    \"\"\"Load a generation file and evaluate its candidates.\"\"\"\n",
        "\n",
        "    with open(filepath, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    candidates = data.get('candidates', [])\n",
        "    metadata = data.get('metadata', {})\n",
        "\n",
        "    print(f\"\\n--- Evaluating: {os.path.basename(filepath)} ---\")\n",
        "    print(f\"Theme: {metadata.get('theme', 'unknown')}\")\n",
        "    print(f\"Form: {metadata.get('form', 'unknown')}\")\n",
        "    print(f\"Num candidates: {len(candidates)}\")\n",
        "\n",
        "    # Evaluate\n",
        "    metrics = evaluate_candidates(candidates, compute_rouge=False)\n",
        "\n",
        "    return {\n",
        "        'filepath': filepath,\n",
        "        'metadata': metadata,\n",
        "        'metrics': metrics,\n",
        "        'candidates': candidates\n",
        "    }\n",
        "\n",
        "# Try to load and evaluate files from notebook 1\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVALUATING GENERATED FILES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "output_dir = 'outputs'\n",
        "if os.path.exists(output_dir):\n",
        "    generation_files = [\n",
        "        f for f in os.listdir(output_dir)\n",
        "        if f.startswith('generation_') and f.endswith('.json')\n",
        "    ]\n",
        "\n",
        "    evaluation_results = []\n",
        "\n",
        "    for filename in generation_files[:3]:  # Evaluate first 3 files\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "        try:\n",
        "            result = load_and_evaluate_generation_file(filepath)\n",
        "            evaluation_results.append(result)\n",
        "\n",
        "            print(\"\\nMetrics:\")\n",
        "            for key, value in sorted(result['metrics'].items()):\n",
        "                if isinstance(value, float):\n",
        "                    print(f\"  {key:20s}: {value:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating {filename}: {e}\")\n",
        "else:\n",
        "    print(\"No outputs directory found. Creating sample evaluation...\")\n",
        "    evaluation_results = []\n",
        "\n",
        "\n",
        "def compare_generation_runs(results: List[Dict]) -> pd.DataFrame:\n",
        "    \"\"\"Compare metrics across multiple generation runs.\"\"\"\n",
        "\n",
        "    if not results:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    comparison_data = []\n",
        "    for result in results:\n",
        "        row = {\n",
        "            'file': os.path.basename(result['filepath']),\n",
        "            'theme': result['metadata'].get('theme', 'unknown'),\n",
        "            'form': result['metadata'].get('form', 'unknown'),\n",
        "            'num_candidates': len(result['candidates'])\n",
        "        }\n",
        "        row.update(result['metrics'])\n",
        "        comparison_data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(comparison_data)\n",
        "    return df\n",
        "\n",
        "if evaluation_results:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"COMPARISON ACROSS RUNS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    comparison_df = compare_generation_runs(evaluation_results)\n",
        "    print(\"\\n\", comparison_df.to_string(index=False))\n",
        "\n",
        "    # Save comparison\n",
        "    os.makedirs('data/training', exist_ok=True) # Add this line to create the directory\n",
        "    comparison_df.to_csv('data/training/generation_comparison.csv', index=False)\n",
        "    print(\"\\n✓ Saved comparison to data/training/generation_comparison.csv\")\n",
        "\n",
        "\n",
        "def visualize_metrics(metrics_dict: Dict[str, float], title: str = \"Metrics\"):\n",
        "    \"\"\"Visualize metrics as a bar chart.\"\"\"\n",
        "\n",
        "    # Select key metrics to visualize\n",
        "    key_metrics = [\n",
        "        'distinct_1', 'distinct_2', 'distinct_3',\n",
        "        'self_bleu', 'type_token_ratio'\n",
        "    ]\n",
        "\n",
        "    available_metrics = {k: v for k, v in metrics_dict.items() if k in key_metrics}\n",
        "\n",
        "    if not available_metrics:\n",
        "        print(\"No key metrics available for visualization\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    keys = list(available_metrics.keys())\n",
        "    values = list(available_metrics.values())\n",
        "\n",
        "    bars = plt.bar(range(len(keys)), values, color='steelblue', alpha=0.7)\n",
        "    plt.xticks(range(len(keys)), keys, rotation=45, ha='right')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title(title)\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.3f}',\n",
        "                ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return plt.gcf()\n",
        "\n",
        "# Visualize metrics from comprehensive evaluation\n",
        "print(\"\\n--- Visualizing Metrics ---\")\n",
        "fig = visualize_metrics(all_metrics, \"Diversity & Quality Metrics\")\n",
        "if fig:\n",
        "    os.makedirs('data/training', exist_ok=True) # Add this line to create the directory\n",
        "    fig.savefig('data/training/metrics_visualization.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"✓ Saved metrics visualization\")\n",
        "\n",
        "\n",
        "def evaluate_ranking_quality(ranked_file: str) -> Dict:\n",
        "    \"\"\"Evaluate the quality of a ranking.\"\"\"\n",
        "\n",
        "    with open(ranked_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    ranked_candidates = data['ranked_candidates']\n",
        "\n",
        "    # Extract texts and scores\n",
        "    texts = [c['text'] for c in ranked_candidates]\n",
        "    scores = [c['score'] for c in ranked_candidates]\n",
        "\n",
        "    # Check if scores are monotonically decreasing\n",
        "    is_monotonic = all(scores[i] >= scores[i+1] for i in range(len(scores)-1))\n",
        "\n",
        "    # Score spread (how much variation in scores)\n",
        "    score_spread = max(scores) - min(scores)\n",
        "\n",
        "    # Evaluate diversity of top-k\n",
        "    top_5_texts = texts[:5]\n",
        "    top_5_metrics = compute_all_distinct_metrics(top_5_texts)\n",
        "\n",
        "    return {\n",
        "        'num_candidates': len(texts),\n",
        "        'is_monotonic': is_monotonic,\n",
        "        'score_range': (min(scores), max(scores)),\n",
        "        'score_spread': score_spread,\n",
        "        'score_mean': np.mean(scores),\n",
        "        'score_std': np.std(scores),\n",
        "        'top_5_distinct_1': top_5_metrics['distinct_1'],\n",
        "        'top_5_distinct_2': top_5_metrics['distinct_2']\n",
        "    }\n",
        "\n",
        "# Try to evaluate ranking files\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVALUATING RANKING QUALITY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if os.path.exists('outputs'):\n",
        "    ranking_files = [\n",
        "        f for f in os.listdir('outputs')\n",
        "        if f.startswith('ranked_') and f.endswith('.json')\n",
        "    ]\n",
        "\n",
        "    for filename in ranking_files[:2]:\n",
        "        filepath = os.path.join('outputs', filename)\n",
        "        try:\n",
        "            print(f\"\\n--- {filename} ---\")\n",
        "            ranking_eval = evaluate_ranking_quality(filepath)\n",
        "\n",
        "            for key, value in ranking_eval.items():\n",
        "                if isinstance(value, tuple):\n",
        "                    print(f\"  {key}: {value}\")\n",
        "                elif isinstance(value, float):\n",
        "                    print(f\"  {key}: {value:.4f}\")\n",
        "                else:\n",
        "                    print(f\"  {key}: {value}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "class EvaluationReport:\n",
        "    \"\"\"Generate comprehensive evaluation reports.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.results = []\n",
        "\n",
        "    def add_evaluation(self, name: str, candidates: List[str], metadata: Dict = None):\n",
        "        \"\"\"Add an evaluation run.\"\"\"\n",
        "        metrics = evaluate_candidates(candidates, compute_rouge=False)\n",
        "\n",
        "        self.results.append({\n",
        "            'name': name,\n",
        "            'metadata': metadata or {},\n",
        "            'metrics': metrics,\n",
        "            'num_candidates': len(candidates)\n",
        "        })\n",
        "\n",
        "    def generate_summary(self) -> pd.DataFrame:\n",
        "        \"\"\"Generate summary DataFrame.\"\"\"\n",
        "        if not self.results:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        data = []\n",
        "        for result in self.results:\n",
        "            row = {\n",
        "                'name': result['name'],\n",
        "                'num_candidates': result['num_candidates'],\n",
        "                **result['metrics']\n",
        "            }\n",
        "            data.append(row)\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def save_report(self, filepath: str = 'data/training/evaluation_report.json'):\n",
        "        \"\"\"Save full report to JSON.\"\"\"\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True) # Add this line to create the directory\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump({\n",
        "                'timestamp': pd.Timestamp.now().isoformat(),\n",
        "                'num_evaluations': len(self.results),\n",
        "                'results': self.results\n",
        "            }, f, indent=2)\n",
        "\n",
        "        print(f\"✓ Saved evaluation report to {filepath}\")\n",
        "\n",
        "# Create sample report\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"GENERATING EVALUATION REPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "report = EvaluationReport()\n",
        "\n",
        "# Add evaluations\n",
        "report.add_evaluation(\n",
        "    \"Diverse Generation\",\n",
        "    test_texts_diverse,\n",
        "    {'description': 'Highly diverse candidate set'}\n",
        ")\n",
        "\n",
        "report.add_evaluation(\n",
        "    \"Repetitive Generation\",\n",
        "    test_texts_repetitive,\n",
        "    {'description': 'Low diversity candidate set'}\n",
        ")\n",
        "\n",
        "# Generate summary\n",
        "summary_df = report.generate_summary()\n",
        "print(\"\\n\", summary_df.to_string(index=False))\n",
        "\n",
        "# Save report\n",
        "report.save_report()\n",
        "\n",
        "\n",
        "def analyze_metric_correlations(results: List[Dict]) -> pd.DataFrame:\n",
        "    \"\"\"Analyze correlations between different metrics.\"\"\"\n",
        "\n",
        "    if len(results) < 3:\n",
        "        print(\"Need at least 3 evaluation runs to analyze correlations\")\n",
        "        return None\n",
        "\n",
        "    # Extract metrics into DataFrame\n",
        "    metric_data = []\n",
        "    for result in results:\n",
        "        metric_data.append(result['metrics'])\n",
        "\n",
        "    df = pd.DataFrame(metric_data)\n",
        "\n",
        "    # Select numeric columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    # Compute correlation matrix\n",
        "    corr_matrix = df[numeric_cols].corr()\n",
        "\n",
        "    # Visualize\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
        "                center=0, square=True, linewidths=1)\n",
        "    plt.title('Metric Correlations')\n",
        "    plt.tight_layout()\n",
        "    os.makedirs('data/training', exist_ok=True) # Add this line to create the directory\n",
        "    plt.savefig('data/training/metric_correlations.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"✓ Saved correlation heatmap\")\n",
        "\n",
        "    return corr_matrix\n",
        "\n",
        "\n",
        "def export_evaluation_functions(output_file: str = 'utils/evaluation_utils.py'):\n",
        "    \"\"\"\n",
        "    Export evaluation functions for use in main codebase.\n",
        "    This would contain all the evaluation functions defined above.\n",
        "    \"\"\"\n",
        "\n",
        "    os.makedirs('utils', exist_ok=True)\n",
        "\n",
        "    functions_code = '''\"\"\"\n",
        "Evaluation utilities for reciprocal poetry generation.\n",
        "Generated from notebook 05.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from typing import List, Dict, Optional\n",
        "from collections import Counter\n",
        "import re\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    \"\"\"Simple tokenization.\"\"\"\n",
        "    tokens = re.findall(r'\\\\b\\\\w+\\\\b', text.lower())\n",
        "    return tokens\n",
        "\n",
        "def compute_distinct_n(texts: List[str], n: int = 1) -> float:\n",
        "    \"\"\"Compute Distinct-N metric for lexical diversity.\"\"\"\n",
        "    all_ngrams = []\n",
        "\n",
        "    for text in texts:\n",
        "        tokens = tokenize(text)\n",
        "        ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
        "        all_ngrams.extend(ngrams)\n",
        "\n",
        "    if not all_ngrams:\n",
        "        return 0.0\n",
        "\n",
        "    return len(set(all_ngrams)) / len(all_ngrams)\n",
        "\n",
        "# Add other functions here...\n",
        "'''\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        f.write(functions_code)\n",
        "\n",
        "    print(f\"✓ Exported evaluation utilities to {output_file}\")\n",
        "\n",
        "export_evaluation_functions()\n",
        "\n",
        "print(\"\\n✓ Notebook 5 Complete - Evaluation metrics ready!\")\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(\"You now have complete implementations for:\")\n",
        "print(\"  1. Generation (GPT-2/GPT-Neo)\")\n",
        "print(\"  2. Ranking (preference embeddings + BoW)\")\n",
        "print(\"  3. Profile management (EWA updates)\")\n",
        "print(\"  4. Reward model (MLP trainer)\")\n",
        "print(\"  5. Evaluation metrics (Distinct-N, ROUGE, Self-BLEU)\")\n",
        "print(\"\\nNext step: Convert notebooks to production code structure!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTWRWnPHfrHu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
