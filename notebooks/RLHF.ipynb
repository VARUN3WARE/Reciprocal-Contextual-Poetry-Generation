{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation\n",
    "# !pip install torch transformers trl peft numpy pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83490824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load models\n",
    "print(\"Loading models...\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "# Reference model (frozen copy for KL penalty)\n",
    "ref_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "ref_model.eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "print(\" Models loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531bf32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Reward Model \n",
    "class RewardModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Reward model for RLHF.\n",
    "    Takes text embeddings and predicts scalar reward.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 768,\n",
    "        hidden_dims: List[int] = [256, 128, 64],\n",
    "        dropout: float = 0.2\n",
    "    ):\n",
    "        super(RewardModel, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer (scalar reward, no activation)\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Embeddings (batch_size, embedding_dim)\n",
    "        \n",
    "        Returns:\n",
    "            Rewards (batch_size,)\n",
    "        \"\"\"\n",
    "        return self.network(x).squeeze(-1)\n",
    "    \n",
    "    def get_reward(self, texts: List[str], embedding_model) -> np.ndarray:\n",
    "        \"\"\"Get rewards for texts.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            embeddings = embedding_model.encode(texts)\n",
    "            embeddings_tensor = torch.FloatTensor(embeddings).to(device)\n",
    "            rewards = self.forward(embeddings_tensor)\n",
    "            return rewards.cpu().numpy()\n",
    "\n",
    "# Initialize reward model\n",
    "reward_model = RewardModel(input_dim=768).to(device)\n",
    "print(\" Reward model initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5290d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or Train Reward Model\n",
    "def train_reward_model_from_feedback(\n",
    "    feedback_file: str = 'data/logs/feedback.jsonl',\n",
    "    embedding_model = None,\n",
    "    num_epochs: int = 50,\n",
    "    lr: float = 0.001\n",
    ") -> RewardModel:\n",
    "    \"\"\"\n",
    "    Train reward model from logged feedback.\n",
    "    \n",
    "    Args:\n",
    "        feedback_file: Path to feedback logs\n",
    "        embedding_model: Sentence transformer\n",
    "        num_epochs: Training epochs\n",
    "        lr: Learning rate\n",
    "    \n",
    "    Returns:\n",
    "        Trained reward model\n",
    "    \"\"\"\n",
    "    print(f\"Training reward model from {feedback_file}...\")\n",
    "    \n",
    "    # Load feedback\n",
    "    feedback_data = []\n",
    "    if os.path.exists(feedback_file):\n",
    "        with open(feedback_file, 'r') as f:\n",
    "            for line in f:\n",
    "                entry = json.loads(line.strip())\n",
    "                if entry['event_type'] in ['select', 'rate', 'reject']:\n",
    "                    feedback_data.append(entry)\n",
    "    \n",
    "    if len(feedback_data) < 10:\n",
    "        print(\"Not enough feedback data, using synthetic data...\")\n",
    "        # Create synthetic data\n",
    "        positive_texts = [\n",
    "            \"The gentle breeze whispers through ancient trees\",\n",
    "            \"Moonlight dances on the tranquil lake\",\n",
    "            \"Mountains stand tall in majestic silence\",\n",
    "            \"Stars shimmer across the velvet night sky\",\n",
    "            \"Rivers flow endlessly toward distant shores\"\n",
    "        ]\n",
    "        negative_texts = [\n",
    "            \"The weather is okay today\",\n",
    "            \"There are trees and stuff\",\n",
    "            \"Water is wet and blue\",\n",
    "            \"It's night and there are stars\",\n",
    "            \"Mountains are big rocks\"\n",
    "        ]\n",
    "        \n",
    "        texts = positive_texts + negative_texts\n",
    "        ratings = [0.9] * len(positive_texts) + [0.2] * len(negative_texts)\n",
    "    else:\n",
    "        texts = [e['data']['text'] for e in feedback_data]\n",
    "        ratings = []\n",
    "        for e in feedback_data:\n",
    "            if 'rating' in e['data']:\n",
    "                ratings.append(e['data']['rating'])\n",
    "            elif e['event_type'] == 'select':\n",
    "                ratings.append(e['data'].get('score', 0.8))\n",
    "            elif e['event_type'] == 'reject':\n",
    "                ratings.append(0.2)\n",
    "            else:\n",
    "                ratings.append(0.5)\n",
    "    \n",
    "    # Compute embeddings\n",
    "    embeddings = embedding_model.encode(texts)\n",
    "    X = torch.FloatTensor(embeddings).to(device)\n",
    "    y = torch.FloatTensor(ratings).to(device)\n",
    "    \n",
    "    # Create reward model\n",
    "    model = RewardModel(input_dim=embeddings.shape[1]).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Train\n",
    "    print(f\"Training on {len(texts)} samples...\")\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(X)\n",
    "        loss = criterion(predictions, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "    \n",
    "    print(f\"✓ Reward model trained (best loss: {best_loss:.4f})\")\n",
    "    return model\n",
    "\n",
    "# Train reward model\n",
    "reward_model = train_reward_model_from_feedback(\n",
    "    embedding_model=embedding_model,\n",
    "    num_epochs=50\n",
    ")\n",
    "\n",
    "# Test reward model\n",
    "print(\"\\n--- Testing Reward Model ---\")\n",
    "test_texts = [\n",
    "    \"The majestic mountain rises above the clouds\",\n",
    "    \"There is a mountain\",\n",
    "    \"Silver moonlight illuminates the quiet forest\",\n",
    "    \"The moon is in the sky\"\n",
    "]\n",
    "\n",
    "rewards = reward_model.get_reward(test_texts, embedding_model)\n",
    "for text, reward in zip(test_texts, rewards):\n",
    "    print(f\"Reward: {reward:.3f} | {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320bac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Components - Value Function\n",
    "class ValueHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Value function for PPO.\n",
    "    Estimates V(s) for a given state (hidden states).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 768):\n",
    "        super(ValueHead, self).__init__()\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        Returns:\n",
    "            values: (batch, seq_len)\n",
    "        \"\"\"\n",
    "        return self.value_head(hidden_states).squeeze(-1)\n",
    "\n",
    "# Create value function\n",
    "value_model = ValueHead(input_dim=768).to(device)\n",
    "print(\" Value function initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3211484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Data Structures\n",
    "@dataclass\n",
    "class PPOExperience:\n",
    "    \"\"\"Single experience for PPO training.\"\"\"\n",
    "    query: str\n",
    "    response: str\n",
    "    response_tokens: torch.Tensor\n",
    "    log_probs: torch.Tensor\n",
    "    values: torch.Tensor\n",
    "    reward: float\n",
    "    advantages: Optional[torch.Tensor] = None\n",
    "    returns: Optional[torch.Tensor] = None\n",
    "\n",
    "class PPOMemory:\n",
    "    \"\"\"Experience replay buffer for PPO.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.experiences = []\n",
    "    \n",
    "    def add(self, experience: PPOExperience):\n",
    "        self.experiences.append(experience)\n",
    "    \n",
    "    def compute_advantages(self, gamma: float = 0.99, lam: float = 0.95):\n",
    "        \"\"\"Compute GAE advantages for all experiences.\"\"\"\n",
    "        for exp in self.experiences:\n",
    "            seq_len = len(exp.values)\n",
    "            \n",
    "            # Compute TD errors\n",
    "            rewards_to_go = torch.zeros(seq_len, device=device)\n",
    "            advantages = torch.zeros(seq_len, device=device)\n",
    "            \n",
    "            # Last reward is the final reward from reward model\n",
    "            running_return = exp.reward\n",
    "            running_advantage = 0\n",
    "            \n",
    "            for t in reversed(range(seq_len)):\n",
    "                # TD error\n",
    "                if t == seq_len - 1:\n",
    "                    td_error = exp.reward - exp.values[t]\n",
    "                else:\n",
    "                    td_error = 0 + gamma * exp.values[t + 1] - exp.values[t]\n",
    "                \n",
    "                # GAE\n",
    "                running_advantage = td_error + gamma * lam * running_advantage\n",
    "                advantages[t] = running_advantage\n",
    "                \n",
    "                # Returns\n",
    "                rewards_to_go[t] = running_return\n",
    "                running_return = gamma * running_return\n",
    "            \n",
    "            exp.advantages = advantages\n",
    "            exp.returns = rewards_to_go\n",
    "    \n",
    "    def get_batch(self):\n",
    "        \"\"\"Get all experiences as batch.\"\"\"\n",
    "        return self.experiences\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear memory.\"\"\"\n",
    "        self.experiences = []\n",
    "\n",
    "print(\" PPO memory and experience structures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97719983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Rollout - Generate and Collect\n",
    "def generate_with_values(\n",
    "    model: GPT2LMHeadModel,\n",
    "    value_model: ValueHead,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    prompts: List[str],\n",
    "    max_length: int = 40,\n",
    "    temperature: float = 0.9,\n",
    "    top_k: int = 50\n",
    ") -> List[Tuple[str, torch.Tensor, torch.Tensor, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Generate responses and collect log probs and values.\n",
    "    \n",
    "    Args:\n",
    "        model: Policy model (GPT-2)\n",
    "        value_model: Value function\n",
    "        tokenizer: Tokenizer\n",
    "        prompts: List of prompts\n",
    "        max_length: Max generation length\n",
    "        temperature: Sampling temperature\n",
    "        top_k: Top-k sampling\n",
    "    \n",
    "    Returns:\n",
    "        List of (response_text, response_tokens, log_probs, values)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    value_model.eval()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        # Encode prompt\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "        prompt_len = input_ids.shape[1]\n",
    "        \n",
    "        # Storage\n",
    "        all_log_probs = []\n",
    "        all_values = []\n",
    "        \n",
    "        # Generate token by token\n",
    "        generated = input_ids\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step in range(max_length):\n",
    "                # Forward pass\n",
    "                outputs = model(generated, output_hidden_states=True, return_dict=True)\n",
    "                hidden_states = outputs.hidden_states[-1]  # Last layer\n",
    "                logits = outputs.logits[:, -1, :]  # Next token logits\n",
    "                \n",
    "                # Get value for current state\n",
    "                value = value_model(hidden_states)[:, -1]\n",
    "                \n",
    "                # Sample next token\n",
    "                logits = logits / temperature\n",
    "                top_k_logits, top_k_indices = torch.topk(logits, top_k, dim=-1)\n",
    "                probs = F.softmax(top_k_logits, dim=-1)\n",
    "                \n",
    "                # Sample\n",
    "                next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "                next_token = top_k_indices.gather(-1, next_token_idx)\n",
    "                \n",
    "                # Compute log prob of sampled token\n",
    "                log_prob = F.log_softmax(top_k_logits, dim=-1)\n",
    "                token_log_prob = log_prob.gather(-1, next_token_idx)\n",
    "                \n",
    "                # Store\n",
    "                all_log_probs.append(token_log_prob.squeeze())\n",
    "                all_values.append(value.squeeze())\n",
    "                \n",
    "                # Append token\n",
    "                generated = torch.cat([generated, next_token], dim=-1)\n",
    "                \n",
    "                # Check for EOS\n",
    "                if next_token.item() == tokenizer.eos_token_id:\n",
    "                    break\n",
    "        \n",
    "        # Decode response (excluding prompt)\n",
    "        response_tokens = generated[0, prompt_len:]\n",
    "        response_text = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        # Stack tensors\n",
    "        log_probs_tensor = torch.stack(all_log_probs)\n",
    "        values_tensor = torch.stack(all_values)\n",
    "        \n",
    "        results.append((response_text, response_tokens, log_probs_tensor, values_tensor))\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Rollout generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0016bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PPO Training Step\n",
    "def ppo_training_step(\n",
    "    model: GPT2LMHeadModel,\n",
    "    value_model: ValueHead,\n",
    "    ref_model: GPT2LMHeadModel,\n",
    "    memory: PPOMemory,\n",
    "    optimizer_policy: torch.optim.Optimizer,\n",
    "    optimizer_value: torch.optim.Optimizer,\n",
    "    clip_eps: float = 0.2,\n",
    "    kl_coef: float = 0.1,\n",
    "    value_coef: float = 0.5,\n",
    "    entropy_coef: float = 0.01,\n",
    "    num_epochs: int = 4\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Perform PPO update on collected experiences.\n",
    "    \n",
    "    Args:\n",
    "        model: Policy model to train\n",
    "        value_model: Value function to train\n",
    "        ref_model: Reference model (frozen)\n",
    "        memory: Experience buffer\n",
    "        optimizer_policy: Optimizer for policy\n",
    "        optimizer_value: Optimizer for value\n",
    "        clip_eps: PPO clipping epsilon\n",
    "        kl_coef: KL penalty coefficient\n",
    "        value_coef: Value loss coefficient\n",
    "        entropy_coef: Entropy bonus coefficient\n",
    "        num_epochs: PPO epochs per batch\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of training metrics\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    value_model.train()\n",
    "    \n",
    "    # Compute advantages\n",
    "    memory.compute_advantages()\n",
    "    \n",
    "    experiences = memory.get_batch()\n",
    "    \n",
    "    total_policy_loss = 0\n",
    "    total_value_loss = 0\n",
    "    total_kl = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for exp in experiences:\n",
    "            # Reconstruct full input (query + response)\n",
    "            query_tokens = gpt2_tokenizer.encode(exp.query, return_tensors='pt').to(device)\n",
    "            full_tokens = torch.cat([query_tokens, exp.response_tokens.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            # Forward pass through policy\n",
    "            outputs = model(full_tokens, output_hidden_states=True, return_dict=True)\n",
    "            hidden_states = outputs.hidden_states[-1]\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get log probs for response tokens\n",
    "            response_len = exp.response_tokens.shape[0]\n",
    "            response_logits = logits[0, -response_len-1:-1, :]  # Shift by 1\n",
    "            log_probs_new = F.log_softmax(response_logits, dim=-1)\n",
    "            \n",
    "            # Gather log probs for actual tokens\n",
    "            token_log_probs_new = log_probs_new.gather(\n",
    "                1, exp.response_tokens.unsqueeze(1)\n",
    "            ).squeeze()\n",
    "            \n",
    "            # Compute ratio for PPO\n",
    "            ratio = torch.exp(token_log_probs_new - exp.log_probs.detach())\n",
    "            \n",
    "            # Clipped surrogate loss\n",
    "            advantages = exp.advantages.detach()\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "            \n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # KL penalty with reference model\n",
    "            with torch.no_grad():\n",
    "                ref_outputs = ref_model(full_tokens)\n",
    "                ref_logits = ref_outputs.logits\n",
    "                ref_response_logits = ref_logits[0, -response_len-1:-1, :]\n",
    "                ref_log_probs = F.log_softmax(ref_response_logits, dim=-1)\n",
    "                ref_token_log_probs = ref_log_probs.gather(\n",
    "                    1, exp.response_tokens.unsqueeze(1)\n",
    "                ).squeeze()\n",
    "            \n",
    "            kl_penalty = (token_log_probs_new - ref_token_log_probs).mean()\n",
    "            \n",
    "            # Entropy bonus (encourage exploration)\n",
    "            probs = F.softmax(response_logits, dim=-1)\n",
    "            entropy = -(probs * log_probs_new).sum(dim=-1).mean()\n",
    "            \n",
    "            # Value loss\n",
    "            response_hidden = hidden_states[0, -response_len-1:-1, :]\n",
    "            values_new = value_model(response_hidden.unsqueeze(0)).squeeze()\n",
    "            value_loss = F.mse_loss(values_new, exp.returns.detach())\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = (\n",
    "                policy_loss +\n",
    "                kl_coef * kl_penalty -\n",
    "                entropy_coef * entropy +\n",
    "                value_coef * value_loss\n",
    "            )\n",
    "            \n",
    "            # Backward\n",
    "            optimizer_policy.zero_grad()\n",
    "            optimizer_value.zero_grad()\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(value_model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer_policy.step()\n",
    "            optimizer_value.step()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            total_policy_loss += policy_loss.item()\n",
    "            total_value_loss += value_loss.item()\n",
    "            total_kl += kl_penalty.item()\n",
    "    \n",
    "    n = len(experiences) * num_epochs\n",
    "    return {\n",
    "        'policy_loss': total_policy_loss / n,\n",
    "        'value_loss': total_value_loss / n,\n",
    "        'kl_divergence': total_kl / n\n",
    "    }\n",
    "\n",
    "print(\" PPO training step defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5977fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLHF Training Loop\n",
    "def train_rlhf(\n",
    "    model: GPT2LMHeadModel,\n",
    "    value_model: ValueHead,\n",
    "    ref_model: GPT2LMHeadModel,\n",
    "    reward_model: RewardModel,\n",
    "    embedding_model,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    prompts: List[str],\n",
    "    num_iterations: int = 10,\n",
    "    batch_size: int = 4,\n",
    "    lr_policy: float = 1e-5,\n",
    "    lr_value: float = 1e-4\n",
    "):\n",
    "    \"\"\"\n",
    "    Main RLHF training loop with PPO.\n",
    "    \n",
    "    Args:\n",
    "        model: Policy model to train\n",
    "        value_model: Value function\n",
    "        ref_model: Reference model (frozen)\n",
    "        reward_model: Reward model\n",
    "        embedding_model: For computing rewards\n",
    "        tokenizer: Tokenizer\n",
    "        prompts: Training prompts\n",
    "        num_iterations: Number of training iterations\n",
    "        batch_size: Prompts per iteration\n",
    "        lr_policy: Learning rate for policy\n",
    "        lr_value: Learning rate for value\n",
    "    \"\"\"\n",
    "    # Optimizers\n",
    "    optimizer_policy = torch.optim.Adam(model.parameters(), lr=lr_policy)\n",
    "    optimizer_value = torch.optim.Adam(value_model.parameters(), lr=lr_value)\n",
    "    \n",
    "    print(f\"\\nStarting RLHF training for {num_iterations} iterations\")\n",
    "    print(f\"Batch size: {batch_size}, Policy LR: {lr_policy}, Value LR: {lr_value}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"\\n--- Iteration {iteration + 1}/{num_iterations} ---\")\n",
    "        \n",
    "        # Sample prompts\n",
    "        batch_prompts = np.random.choice(prompts, size=min(batch_size, len(prompts)), replace=False).tolist()\n",
    "        \n",
    "        # Rollout: generate responses\n",
    "        print(\"Generating responses...\")\n",
    "        rollout_results = generate_with_values(\n",
    "            model, value_model, tokenizer,\n",
    "            batch_prompts,\n",
    "            max_length=30\n",
    "        )\n",
    "        \n",
    "        # Create memory\n",
    "        memory = PPOMemory()\n",
    "        \n",
    "        # Compute rewards and store experiences\n",
    "        print(\"Computing rewards...\")\n",
    "        for i, (prompt, (response, resp_tokens, log_probs, values)) in enumerate(zip(batch_prompts, rollout_results)):\n",
    "            full_text = prompt + \" \" + response\n",
    "            reward = reward_model.get_reward([full_text], embedding_model)[0]\n",
    "            \n",
    "            experience = PPOExperience(\n",
    "                query=prompt,\n",
    "                response=response,\n",
    "                response_tokens=resp_tokens,\n",
    "                log_probs=log_probs,\n",
    "                values=values,\n",
    "                reward=reward\n",
    "            )\n",
    "            memory.add(experience)\n",
    "            \n",
    "            print(f\"  [{i+1}] Reward: {reward:.3f} | {prompt} → {response[:50]}...\")\n",
    "        \n",
    "        # PPO update\n",
    "        print(\"Performing PPO update...\")\n",
    "        metrics = ppo_training_step(\n",
    "            model, value_model, ref_model, memory,\n",
    "            optimizer_policy, optimizer_value,\n",
    "            clip_eps=0.2,\n",
    "            kl_coef=0.1,\n",
    "            num_epochs=4\n",
    "        )\n",
    "        \n",
    "        print(f\"  Policy Loss: {metrics['policy_loss']:.4f}\")\n",
    "        print(f\"  Value Loss: {metrics['value_loss']:.4f}\")\n",
    "        print(f\"  KL Divergence: {metrics['kl_divergence']:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (iteration + 1) % 5 == 0:\n",
    "            os.makedirs('models/rlhf', exist_ok=True)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'value_model_state_dict': value_model.state_dict(),\n",
    "                'optimizer_policy': optimizer_policy.state_dict(),\n",
    "                'optimizer_value': optimizer_value.state_dict(),\n",
    "            }, f'models/rlhf/checkpoint_iter_{iteration+1}.pt')\n",
    "            print(f\"  Saved checkpoint\")\n",
    "    \n",
    "    print(\"RLHF training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab4e30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RLHF Training (Small Scale)\n",
    "print(\"TESTING RLHF TRAINING (SMALL SCALE)\")\n",
    "\n",
    "# Prepare training prompts\n",
    "training_prompts = [\n",
    "    \"The mountain\",\n",
    "    \"A gentle breeze\",\n",
    "    \"The moonlight\",\n",
    "    \"In autumn\",\n",
    "    \"The ocean waves\",\n",
    "    \"A quiet moment\",\n",
    "    \"The starlight\",\n",
    "    \"Through the forest\"\n",
    "]\n",
    "\n",
    "# Clone model for training (don't modify original)\n",
    "trainable_model = deepcopy(gpt2_model)\n",
    "trainable_value = ValueHead(input_dim=768).to(device)\n",
    "\n",
    "# Run RLHF for a few iterations\n",
    "print(\"\\nTraining RLHF model...\")\n",
    "train_rlhf(\n",
    "    model=trainable_model,\n",
    "    value_model=trainable_value,\n",
    "    ref_model=ref_model,\n",
    "    reward_model=reward_model,\n",
    "    embedding_model=embedding_model,\n",
    "    tokenizer=gpt2_tokenizer,\n",
    "    prompts=training_prompts,\n",
    "    num_iterations=5,\n",
    "    batch_size=3,\n",
    "    lr_policy=1e-5,\n",
    "    lr_value=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d434217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare Before/After RLHF\n",
    "print(\"COMPARING BEFORE/AFTER RLHF\")\n",
    "\n",
    "test_prompts = [\"The forest\", \"Moonlight falls\", \"A river flows\"]\n",
    "\n",
    "print(\"\\nBEFORE RLHF (Original GPT-2) \")\n",
    "for prompt in test_prompts:\n",
    "    with torch.no_grad():\n",
    "        input_ids = gpt2_tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "        output = gpt2_model.generate(\n",
    "            input_ids,\n",
    "            max_length=input_ids.shape[1] + 30,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            temperature=0.9,\n",
    "            pad_token_id=gpt2_tokenizer.eos_token_id\n",
    "        )\n",
    "        text = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        reward = reward_model.get_reward([text], embedding_model)[0]\n",
    "        print(f\"\\n[Reward: {reward:.3f}] {text}\")\n",
    "\n",
    "print(\"\\n AFTER RLHF (Fine-tuned) \")\n",
    "trainable_model.eval()\n",
    "for prompt in test_prompts:\n",
    "    with torch.no_grad():\n",
    "        input_ids = gpt2_tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "        output = trainable_model.generate(\n",
    "            input_ids,\n",
    "            max_length=input_ids.shape[1] + 30,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            temperature=0.9,\n",
    "            pad_token_id=gpt2_tokenizer.eos_token_id\n",
    "        )\n",
    "        text = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        reward = reward_model.get_reward([text], embedding_model)[0]\n",
    "        print(f\"\\n[Reward: {reward:.3f}] {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197bedfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  RLHF with User Feedback Integration\n",
    "class RLHFPoetryGenerator:\n",
    "    \"\"\"\n",
    "    RLHF-based poetry generator with reciprocal learning.\n",
    "    Integrates with feedback logging and reward model updates.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model: GPT2LMHeadModel,\n",
    "        tokenizer: GPT2Tokenizer,\n",
    "        embedding_model,\n",
    "        user_id: str\n",
    "    ):\n",
    "        self.base_model = base_model\n",
    "        self.policy_model = deepcopy(base_model)\n",
    "        self.ref_model = deepcopy(base_model)\n",
    "        self.ref_model.eval()\n",
    "        for param in self.ref_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.value_model = ValueHead().to(device)\n",
    "        self.reward_model = RewardModel().to(device)\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedding_model = embedding_model\n",
    "        self.user_id = user_id\n",
    "        \n",
    "        self.feedback_history = []\n",
    "        \n",
    "        print(f\"✓ RLHF Poetry Generator initialized for user {user_id}\")\n",
    "    \n",
    "    def generate(self, prompt: str, max_length: int = 40) -> str:\n",
    "        \"\"\"Generate poetry using current policy.\"\"\"\n",
    "        self.policy_model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "            output = self.policy_model.generate(\n",
    "                input_ids,\n",
    "                max_length=input_ids.shape[1] + max_length,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                temperature=0.9,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def add_feedback(self, text: str, rating: float):\n",
    "        \"\"\"Add user feedback.\"\"\"\n",
    "        self.feedback_history.append({'text': text, 'rating': rating})\n",
    "        print(f\"✓ Added feedback: rating={rating:.2f}\")\n",
    "    \n",
    "    def update_from_feedback(self):\n",
    "        \"\"\"Update reward model and retrain policy.\"\"\"\n",
    "        if len(self.feedback_history) < 5:\n",
    "            print(\"Need at least 5 feedback examples to update\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nUpdating from {len(self.feedback_history)} feedback examples...\")\n",
    "        \n",
    "        # Retrain reward model\n",
    "        texts = [f['text'] for f in self.feedback_history]\n",
    "        ratings = [f['rating'] for f in self.feedback_history]\n",
    "        \n",
    "        embeddings = self.embedding_model.encode(texts)\n",
    "        X = torch.FloatTensor(embeddings).to(device)\n",
    "        y = torch.FloatTensor(ratings).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.reward_model.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        for epoch in range(50):\n",
    "            optimizer.zero_grad()\n",
    "            preds = self.reward_model(X)\n",
    "            loss = criterion(preds, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"✓ Reward model updated (final loss: {loss.item():.4f})\")\n",
    "        \n",
    "        # Retrain policy with RLHF\n",
    "        prompts = [\"The mountain\", \"A gentle breeze\", \"The moonlight\", \"In autumn\"]\n",
    "        train_rlhf(\n",
    "            model=self.policy_model,\n",
    "            value_model=self.value_model,\n",
    "            ref_model=self.ref_model,\n",
    "            reward_model=self.reward_model,\n",
    "            embedding_model=self.embedding_model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            prompts=prompts,\n",
    "            num_iterations=3,\n",
    "            batch_size=2,\n",
    "            lr_policy=1e-5,\n",
    "            lr_value=1e-4\n",
    "        )\n",
    "        \n",
    "        print(\"Policy updated with RLHF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db5a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RLHF Poetry Generator\n",
    "print(\"\\nTesting RLHF Poetry Generator \")\n",
    "rlhf_gen = RLHFPoetryGenerator(\n",
    "    base_model=gpt2_model,\n",
    "    tokenizer=gpt2_tokenizer,\n",
    "    embedding_model=embedding_model,\n",
    "    user_id='bob'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2183fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate initial poems\n",
    "print(\"\\n Initial Generation \")\n",
    "prompts = [\"The stars\", \"A whisper\", \"The dawn\"]\n",
    "initial_poems = []\n",
    "for prompt in prompts:\n",
    "    poem = rlhf_gen.generate(prompt, max_length=30)\n",
    "    initial_poems.append(poem)\n",
    "    print(f\"{prompt} → {poem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate user feedback\n",
    "print(\"\\nSimulating User Feedback \")\n",
    "# User likes poetic, flowing language\n",
    "rlhf_gen.add_feedback(\"The gentle breeze whispers through ancient trees\", 0.95)\n",
    "rlhf_gen.add_feedback(\"Moonlight dances on the tranquil lake\", 0.90)\n",
    "rlhf_gen.add_feedback(\"Mountains stand tall in majestic silence\", 0.88)\n",
    "rlhf_gen.add_feedback(\"Stars shimmer across the velvet night\", 0.92)\n",
    "rlhf_gen.add_feedback(\"Rivers flow endlessly toward distant shores\", 0.89)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971db8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User dislikes plain descriptions\n",
    "rlhf_gen.add_feedback(\"The weather is okay today\", 0.15)\n",
    "rlhf_gen.add_feedback(\"There are trees and stuff\", 0.10)\n",
    "rlhf_gen.add_feedback(\"Water is wet and blue\", 0.12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update models\n",
    "rlhf_gen.update_from_feedback()\n",
    "\n",
    "# Generate after feedback\n",
    "print(\"\\n--- After Feedback Update ---\")\n",
    "for prompt in prompts:\n",
    "    poem = rlhf_gen.generate(prompt, max_length=30)\n",
    "    print(f\"{prompt} → {poem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cef0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLHF Metrics and Evaluation\n",
    "def evaluate_rlhf_model(\n",
    "    model: GPT2LMHeadModel,\n",
    "    ref_model: GPT2LMHeadModel,\n",
    "    reward_model: RewardModel,\n",
    "    embedding_model,\n",
    "    tokenizer: GPT2Tokenizer,\n",
    "    test_prompts: List[str],\n",
    "    num_samples: int = 5\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate RLHF-trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained policy model\n",
    "        ref_model: Reference model\n",
    "        reward_model: Reward model\n",
    "        embedding_model: For computing rewards\n",
    "        tokenizer: Tokenizer\n",
    "        test_prompts: Test prompts\n",
    "        num_samples: Samples per prompt\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_rewards = []\n",
    "    all_kl_divs = []\n",
    "    all_texts = []\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        for _ in range(num_samples):\n",
    "            # Generate with trained model\n",
    "            with torch.no_grad():\n",
    "                input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "                \n",
    "                # Policy model\n",
    "                output_policy = model.generate(\n",
    "                    input_ids,\n",
    "                    max_length=input_ids.shape[1] + 30,\n",
    "                    do_sample=True,\n",
    "                    top_k=50,\n",
    "                    temperature=0.9,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    output_scores=True,\n",
    "                    return_dict_in_generate=True\n",
    "                )\n",
    "                \n",
    "                text = tokenizer.decode(output_policy.sequences[0], skip_special_tokens=True)\n",
    "                all_texts.append(text)\n",
    "                \n",
    "                # Compute reward\n",
    "                reward = reward_model.get_reward([text], embedding_model)[0]\n",
    "                all_rewards.append(reward)\n",
    "                \n",
    "                # Compute KL divergence with reference model\n",
    "                policy_logits = torch.stack(output_policy.scores, dim=1)  # (1, seq_len, vocab)\n",
    "                \n",
    "                # Get reference logits\n",
    "                ref_output = ref_model(output_policy.sequences, return_dict=True)\n",
    "                ref_logits = ref_output.logits[:, :-1, :]  # Shift\n",
    "                \n",
    "                # Match dimensions\n",
    "                min_len = min(policy_logits.shape[1], ref_logits.shape[1])\n",
    "                policy_logits = policy_logits[:, :min_len, :]\n",
    "                ref_logits = ref_logits[:, :min_len, :]\n",
    "                \n",
    "                # KL divergence\n",
    "                policy_probs = F.softmax(policy_logits, dim=-1)\n",
    "                ref_probs = F.softmax(ref_logits, dim=-1)\n",
    "                kl = F.kl_div(\n",
    "                    policy_probs.log(),\n",
    "                    ref_probs,\n",
    "                    reduction='batchmean'\n",
    "                )\n",
    "                all_kl_divs.append(kl.item())\n",
    "    \n",
    "    # Compute diversity\n",
    "    from collections import Counter\n",
    "    all_words = []\n",
    "    for text in all_texts:\n",
    "        words = text.lower().split()\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    unique_words = len(set(all_words))\n",
    "    total_words = len(all_words)\n",
    "    diversity = unique_words / total_words if total_words > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(all_rewards),\n",
    "        'std_reward': np.std(all_rewards),\n",
    "        'mean_kl': np.mean(all_kl_divs),\n",
    "        'std_kl': np.std(all_kl_divs),\n",
    "        'diversity': diversity,\n",
    "        'num_samples': len(all_texts)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa21886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trained model\n",
    "test_prompts_eval = [\"The mountain\", \"A gentle breeze\", \"The moonlight\", \"In autumn\", \"The ocean\"]\n",
    "\n",
    "metrics_before = evaluate_rlhf_model(\n",
    "    model=gpt2_model,\n",
    "    ref_model=ref_model,\n",
    "    reward_model=reward_model,\n",
    "    embedding_model=embedding_model,\n",
    "    tokenizer=gpt2_tokenizer,\n",
    "    test_prompts=test_prompts_eval,\n",
    "    num_samples=3\n",
    ")\n",
    "\n",
    "metrics_after = evaluate_rlhf_model(\n",
    "    model=trainable_model,\n",
    "    ref_model=ref_model,\n",
    "    reward_model=reward_model,\n",
    "    embedding_model=embedding_model,\n",
    "    tokenizer=gpt2_tokenizer,\n",
    "    test_prompts=test_prompts_eval,\n",
    "    num_samples=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249231fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n BEFORE RLHF \")\n",
    "for key, value in metrics_before.items():\n",
    "    print(f\"  {key:15s}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n AFTER RLHF \")\n",
    "for key, value in metrics_after.items():\n",
    "    print(f\"  {key:15s}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n IMPROVEMENT \")\n",
    "print(f\"  Reward :  {metrics_after['mean_reward'] - metrics_before['mean_reward']:+.4f}\")\n",
    "print(f\"  KL :      {metrics_after['mean_kl'] - metrics_before['mean_kl']:+.4f}\")\n",
    "print(f\"  Diversity : {metrics_after['diversity'] - metrics_before['diversity']:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d971a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save RLHF Models\n",
    "print(\"SAVING RLHF MODELS\")\n",
    "\n",
    "os.makedirs('models/rlhf', exist_ok=True)\n",
    "\n",
    "# Save policy model\n",
    "trainable_model.save_pretrained('models/rlhf/policy_model')\n",
    "gpt2_tokenizer.save_pretrained('models/rlhf/policy_model')\n",
    "\n",
    "# Save value model\n",
    "torch.save({\n",
    "    'model_state_dict': trainable_value.state_dict(),\n",
    "}, 'models/rlhf/value_model.pt')\n",
    "\n",
    "# Save reward model\n",
    "torch.save({\n",
    "    'model_state_dict': reward_model.state_dict(),\n",
    "}, 'models/rlhf/reward_model.pt')\n",
    "\n",
    "print(\" Saved policy model to models/rlhf/policy_model/\")\n",
    "print(\" Saved value model to models/rlhf/value_model.pt\")\n",
    "print(\" Saved reward model to models/rlhf/reward_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07647ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experiment results\n",
    "experiment_results = {\n",
    "    'metrics_before': metrics_before,\n",
    "    'metrics_after': metrics_after,\n",
    "    'training_config': {\n",
    "        'num_iterations': 5,\n",
    "        'batch_size': 3,\n",
    "        'lr_policy': 1e-5,\n",
    "        'lr_value': 1e-4,\n",
    "        'clip_eps': 0.2,\n",
    "        'kl_coef': 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "os.makedirs('outputs/rlhf', exist_ok=True)\n",
    "with open('outputs/rlhf/experiment_results.json', 'w') as f:\n",
    "    json.dump(experiment_results, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
