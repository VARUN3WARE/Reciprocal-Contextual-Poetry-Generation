{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch sentence-transformers numpy pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9yVxGvgSG4_",
        "outputId": "4267970d-1e01-4a7c-c56e-f4ff79a3371e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoModelForCausalLM, AutoTokenizer\n",
        "import json\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du67Mee5SJ8s",
        "outputId": "88eb14b3-c8ea-48c3-b3eb-449af8143f4c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_model(model_name: str = 'gpt2') -> Tuple[object, object]:\n",
        "    \"\"\"\n",
        "    Load a causal LM for poetry generation.\n",
        "\n",
        "    Args:\n",
        "        model_name: 'gpt2', 'gpt2-medium', 'EleutherAI/gpt-neo-125M', etc.\n",
        "\n",
        "    Returns:\n",
        "        model, tokenizer\n",
        "    \"\"\"\n",
        "    print(f\"Loading {model_name}...\")\n",
        "\n",
        "    if 'gpt2' in model_name:\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Set pad token if not present\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(f\"Model loaded successfully on {device}\")\n",
        "    return model, tokenizer\n",
        "\n",
        "# Load the model (run this cell)\n",
        "model, tokenizer = load_model('gpt2')  # i will mostly change thsi  to 'gpt2-medium' or 'EleutherAI/gpt-neo-125M' if needed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy3FbEc8SVNB",
        "outputId": "c89da019-a7f9-4449-e7ed-bac7e123515b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading gpt2...\n",
            "Model loaded successfully on cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "POETRY_PROMPTS = {\n",
        "    'haiku': \"Write a haiku about {theme}:\\n\",\n",
        "    'sonnet': \"A sonnet about {theme}:\\n\",\n",
        "    'free_verse': \"A poem about {theme}:\\n\",\n",
        "    'limerick': \"A limerick about {theme}:\\n\",\n",
        "    'couplet': \"Write a rhyming couplet about {theme}:\\n\",\n",
        "    'blank': \"{theme}\\n\"  # Minimal prompt is given now fot this one only\n",
        "}\n",
        "\n",
        "def create_prompt(theme: str, form: str = 'free_verse') -> str:\n",
        "    \"\"\"Create a poetry prompt based on theme and form.\"\"\"\n",
        "    template = POETRY_PROMPTS.get(form, POETRY_PROMPTS['free_verse'])\n",
        "    return template.format(theme=theme)\n",
        "\n",
        "# Test prompts\n",
        "test_theme = \"ocean waves\"\n",
        "for form_name, _ in POETRY_PROMPTS.items():\n",
        "    prompt = create_prompt(test_theme, form_name)\n",
        "    print(f\"{form_name:12s}: {prompt[:50]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-STmEY6ySeQs",
        "outputId": "9f401ffa-037b-4b41-f483-8fa50cee6c32"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "haiku       : Write a haiku about ocean waves:\n",
            "...\n",
            "sonnet      : A sonnet about ocean waves:\n",
            "...\n",
            "free_verse  : A poem about ocean waves:\n",
            "...\n",
            "limerick    : A limerick about ocean waves:\n",
            "...\n",
            "couplet     : Write a rhyming couplet about ocean waves:\n",
            "...\n",
            "blank       : ocean waves\n",
            "...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_candidates(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt: str,\n",
        "    num_candidates: int = 10,\n",
        "    max_length: int = 50,\n",
        "    temperature: float = 0.9,\n",
        "    top_p: float = 0.95,\n",
        "    top_k: int = 50,\n",
        "    repetition_penalty: float = 1.2,\n",
        "    seed: Optional[int] = None\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate multiple poetry candidates using nucleus sampling.\n",
        "\n",
        "    Args:\n",
        "        model: Language model\n",
        "        tokenizer: Tokenizer\n",
        "        prompt: Input prompt text\n",
        "        num_candidates: Number of candidates to generate\n",
        "        max_length: Maximum tokens to generate\n",
        "        temperature: Sampling temperature (higher = more random)\n",
        "        top_p: Nucleus sampling threshold\n",
        "        top_k: Top-k sampling limit\n",
        "        repetition_penalty: Penalty for repeating tokens\n",
        "        seed: Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        List of generated text strings\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    # Encode prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    candidates = []\n",
        "\n",
        "    print(f\"Generating {num_candidates} candidates...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(num_candidates):\n",
        "            # Generate with sampling\n",
        "            output = model.generate(\n",
        "                input_ids,\n",
        "                max_length=input_ids.shape[1] + max_length,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                top_k=top_k,\n",
        "                repetition_penalty=repetition_penalty,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                num_return_sequences=1\n",
        "            )\n",
        "\n",
        "            # Decodeing and cleaning  stuff\n",
        "            text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "            # Removing  the prompt from output now\n",
        "            generated_text = text[len(prompt):].strip()\n",
        "            candidates.append(generated_text)\n",
        "\n",
        "            if (i + 1) % 5 == 0:\n",
        "                print(f\"  Generated {i + 1}/{num_candidates}\")\n",
        "\n",
        "    return candidates\n"
      ],
      "metadata": {
        "id": "J7lhn8gjSmvH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 1: Basic generation\n",
        "theme = \"moonlight\"\n",
        "form = \"haiku\"\n",
        "prompt = create_prompt(theme, form)\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "\n",
        "candidates_basic = generate_candidates(\n",
        "    model, tokenizer, prompt,\n",
        "    num_candidates=5,\n",
        "    max_length=30,\n",
        "    temperature=0.8,\n",
        "    top_p=0.9,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"\\n Generated Candidates (Basic) \")\n",
        "for i, text in enumerate(candidates_basic, 1):\n",
        "    print(f\"\\n[{i}] {text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-vFKgCTSxJt",
        "outputId": "a3771219-446a-4292-8a49-11d3871441bb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Write a haiku about moonlight:\n",
            "\n",
            "Generating 5 candidates...\n",
            "  Generated 5/5\n",
            "\n",
            " Generated Candidates (Basic) \n",
            "\n",
            "[1] \"I've been searching the internet for more lunar resources, and am finally here. I want to spend an afternoon reading this book.\"\n",
            "\n",
            "[2] Luna's mind goes to sleep. The dream is too long, her life will never end as it did before… A light shines on the world\n",
            "\n",
            "[3] There is no space in the sky; there are only shadows. The darkness of light comes from below, and its rays cannot reach you because it does\n",
            "\n",
            "[4] There are countless places to view the Moon in one sitting, from your bedroom window through our family room windows. You can always count on us for more\n",
            "\n",
            "[5] This poem is not even for the average joe. This isn't because it's an actual story or that I've never written one before, but\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiment with Different Temperatures\n",
        "print(\"TEMPERATURE EXPERIMENTS\")\n",
        "\n",
        "temperatures = [0.5, 0.8, 1.0, 1.2]\n",
        "theme = \"autumn leaves\"\n",
        "prompt = create_prompt(theme, \"free_verse\")\n",
        "\n",
        "temp_results = {}\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(f\"\\n Temperature = {temp} \")\n",
        "    candidates = generate_candidates(\n",
        "        model, tokenizer, prompt,\n",
        "        num_candidates=3,\n",
        "        max_length=40,\n",
        "        temperature=temp,\n",
        "        top_p=0.95,\n",
        "        seed=42\n",
        "    )\n",
        "    temp_results[temp] = candidates\n",
        "\n",
        "    for i, text in enumerate(candidates, 1):\n",
        "        print(f\"[{i}] {text[:100]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI5kIBe8TNk2",
        "outputId": "070f9031-e4a3-40b9-ba49-66c4f01515f7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEMPERATURE EXPERIMENTS\n",
            "\n",
            " Temperature = 0.5 \n",
            "Generating 3 candidates...\n",
            "[1] \"I've been searching for a place where I can sing. It's hard to find one, but it has always seemed l\n",
            "[2] \"The flower of the spring will fall from me, and I shall be as a dead man in his grave. A cold wind \n",
            "[3] I am a flower, but I have no sense of smell. My body is not my own; it's the garden-tree that gives \n",
            "\n",
            " Temperature = 0.8 \n",
            "Generating 3 candidates...\n",
            "[1] \"They've been around forever. And the trees they call us are like a new dawn... They don't get here \n",
            "[2] As I write this, the moon is in its summer sunlight. A beautiful light from a spring garden blooms b\n",
            "[3] So, I have two options for this fall. For the first one is to play a musical tune (I'm going with \"D\n",
            "\n",
            " Temperature = 1.0 \n",
            "Generating 3 candidates...\n",
            "[1] \"They've been around here in the past, they'd go up like a flying saucer and it's really hard. I alw\n",
            "[2] As I write this, the moon is in its summer sunlight and light. At dawn a spring leaf grows under my \n",
            "[3] So, I have two options for this fall; one is to leave the Winter Forest and get out of it on my own \n",
            "\n",
            " Temperature = 1.2 \n",
            "Generating 3 candidates...\n",
            "[1] \"They've given me the summer, so long ago that my hands had little time.\" –E.J.— \"The winter was lik\n",
            "[2] As I write this, the moon's in his path… A light of red dawning. (Hymn to Spring) \"The sun rises…\" T\n",
            "[3] So, I have two options for this fall; one is to leave the Winter Forest and get out of it on my shou\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_candidates(candidates: List[str], metadata: Dict, filename: str):\n",
        "    \"\"\"Save generated candidates with metadata.\"\"\"\n",
        "    os.makedirs('outputs', exist_ok=True)\n",
        "\n",
        "    data = {\n",
        "        'metadata': metadata,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'candidates': candidates\n",
        "    }\n",
        "\n",
        "    filepath = os.path.join('outputs', filename)\n",
        "    with open(filepath, 'w') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "    print(f\"Saved {len(candidates)} candidates to {filepath}\")\n",
        "\n",
        "# Save results\n",
        "metadata = {\n",
        "    'theme': theme,\n",
        "    'form': form,\n",
        "    'model': 'gpt2',\n",
        "    'temperature': 0.8,\n",
        "    'top_p': 0.9,\n",
        "    'num_candidates': len(candidates_basic)\n",
        "}\n",
        "\n",
        "save_candidates(candidates_basic, metadata, 'generation_test_001.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C18_bK8VTc_d",
        "outputId": "980b3d1a-3455-4553-d064-c809a3db1b51"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 5 candidates to outputs/generation_test_001.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch Generation for Multiple Themes\n",
        "themes_to_test = [\n",
        "    (\"ocean waves\", \"haiku\"),\n",
        "    (\"lost love\", \"sonnet\"),\n",
        "    (\"winter storm\", \"free_verse\"),\n",
        "    (\"dancing fireflies\", \"couplet\")\n",
        "]\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "print(\"BATCH GENERATION FOR MULTIPLE THEMES\")\n",
        "\n",
        "for theme, form in themes_to_test:\n",
        "    print(f\"\\n Theme: {theme} | Form: {form} \")\n",
        "    prompt = create_prompt(theme, form)\n",
        "\n",
        "    candidates = generate_candidates(\n",
        "        model, tokenizer, prompt,\n",
        "        num_candidates=5,\n",
        "        max_length=40,\n",
        "        temperature=0.85,\n",
        "        seed=None  # Random each time\n",
        "    )\n",
        "\n",
        "    all_results[f\"{theme}_{form}\"] = {\n",
        "        'theme': theme,\n",
        "        'form': form,\n",
        "        'candidates': candidates\n",
        "    }\n",
        "\n",
        "    # Show first 2 candidates\n",
        "    for i, text in enumerate(candidates[:2], 1):\n",
        "        print(f\"  [{i}] {text[:80]}\")\n",
        "\n",
        "# Save all results\n",
        "with open('outputs/batch_generation_results.json', 'w') as f:\n",
        "    json.dump(all_results, f, indent=2)\n",
        "\n",
        "print(\"\\nAll batch results saved to outputs/batch_generation_results.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf_1DM4xTklu",
        "outputId": "f5784ded-8162-40b8-8d11-d8f5fa8a9460"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH GENERATION FOR MULTIPLE THEMES\n",
            "\n",
            " Theme: ocean waves | Form: haiku \n",
            "Generating 5 candidates...\n",
            "  Generated 5/5\n",
            "  [1] In the world of Japanese poetry, there's only one kind that works best. In this \n",
            "  [2] I've just finished reading some beautiful, beautifully done Haikyu Hana. It's an\n",
            "\n",
            " Theme: lost love | Form: sonnet \n",
            "Generating 5 candidates...\n",
            "  Generated 5/5\n",
            "  [1] One family of children was devastated when they discovered their loved one disap\n",
            "  [2] I never thought the worst thing that could happen to my life is for someone I kn\n",
            "\n",
            " Theme: winter storm | Form: free_verse \n",
            "Generating 5 candidates...\n",
            "  Generated 5/5\n",
            "  [1] This is like a good season's wind, my little boy. I have just received this one…\n",
            "  [2] As I walk through the woods, a faint breeze blows down to me and whispers \"Winte\n",
            "\n",
            " Theme: dancing fireflies | Form: couplet \n",
            "Generating 5 candidates...\n",
            "  Generated 5/5\n",
            "  [1] The next day you see all the ladies in some new set of clothes who are on their \n",
            "  [2] \"When you ask them where they are going, if I'm gonna dance on their side of the\n",
            "\n",
            "All batch results saved to outputs/batch_generation_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Quality Analysis my basic Metrics\n",
        "def analyze_candidates(candidates: List[str]) -> Dict:\n",
        "    \"\"\"\n",
        "    Basic quality analysis of generated candidates.\n",
        "\n",
        "    Returns metrics like avg length, unique words, etc.\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    metrics = {\n",
        "        'count': len(candidates),\n",
        "        'avg_length': sum(len(c) for c in candidates) / len(candidates) if candidates else 0,\n",
        "        'avg_words': sum(len(c.split()) for c in candidates) / len(candidates) if candidates else 0,\n",
        "        'unique_candidates': len(set(candidates)),\n",
        "        'avg_unique_words': 0\n",
        "    }\n",
        "\n",
        "    # Count unique words per candidate\n",
        "    unique_word_counts = []\n",
        "    for c in candidates:\n",
        "        words = re.findall(r'\\b\\w+\\b', c.lower())\n",
        "        unique_word_counts.append(len(set(words)))\n",
        "\n",
        "    if unique_word_counts:\n",
        "        metrics['avg_unique_words'] = sum(unique_word_counts) / len(unique_word_counts)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Analyze our basic candidates\n",
        "metrics = analyze_candidates(candidates_basic)\n",
        "print(\"\\n Generation Quality Metrics \")\n",
        "for key, value in metrics.items():\n",
        "    print(f\"{key:20s}: {value:.2f}\" if isinstance(value, float) else f\"{key:20s}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l5UMRcGbxLo",
        "outputId": "b7254dd0-2237-4550-fb57-fac54ac83f51"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Generation Quality Metrics \n",
            "count               : 5\n",
            "avg_length          : 133.00\n",
            "avg_words           : 25.20\n",
            "unique_candidates   : 5\n",
            "avg_unique_words    : 25.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LsDfPNM2RmnS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d04db416-33b7-4cb5-b015-bd8e02793d49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INTERACTIVE POETRY GENERATION\n",
            "Enter 'quit' to exit\n",
            "\n",
            "Enter theme (or 'quit'): dancing love\n",
            "Enter form (haiku/sonnet/free_verse/couplet/blank): free verse\n",
            "Number of candidates (default 5): 5\n",
            "\n",
            "Generating 5 candidates for 'dancing love'...\n",
            "Generating 5 candidates...\n",
            "  Generated 5/5\n",
            "\n",
            " Results \n",
            "\n",
            "[1] \"There are many words in English that refer to the relationship of woman and man, but one is often more than what we know. We say no not so much as 'to be loved', or simply how she feels.\" -Linda Thompson (\n",
            "\n",
            "[2] \"I've never met a guy who's less beautiful than me. He'd just go out and do it like he does to other girls.\"\n",
            "\n",
            " \"He said I could be any of these different, but they all wanted him for himself! They\n",
            "\n",
            "[3] So long as I am free of all the pleasures that you have made me, it is not worth my while to sing or dance with anybody. No matter how much better a song its sound can be; no wonder then why people take pleasure in singing\n",
            "\n",
            "[4] The story begins at a school dance, and I am introduced to an old fellow who has gone on as long ago. When we sit up there for the first time together of nearly twenty years he says that it is one day when you have danced with\n",
            "\n",
            "[5] \"I am a little old. I was ten when she left me, and they were all at my bedside.\" It's from the New York Times — by Ann Coulter (@AnnCoulter) September 24—whose father died just before her\n",
            "Enter theme (or 'quit'): quit\n"
          ]
        }
      ],
      "source": [
        "def interactive_generate():\n",
        "    \"\"\"Interactive poetry generation for testing.\"\"\"\n",
        "    print(\"INTERACTIVE POETRY GENERATION\")\n",
        "    print(\"Enter 'quit' to exit\\n\")\n",
        "\n",
        "    while True:\n",
        "        theme = input(\"Enter theme (or 'quit'): \").strip()\n",
        "        if theme.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        form = input(\"Enter form (haiku/sonnet/free_verse/couplet/blank): \").strip() or 'free_verse'\n",
        "        num = input(\"Number of candidates (default 5): \").strip() or '5'\n",
        "\n",
        "        try:\n",
        "            num = int(num)\n",
        "        except:\n",
        "            num = 5\n",
        "\n",
        "        prompt = create_prompt(theme, form)\n",
        "        print(f\"\\nGenerating {num} candidates for '{theme}'...\")\n",
        "\n",
        "        candidates = generate_candidates(\n",
        "            model, tokenizer, prompt,\n",
        "            num_candidates=num,\n",
        "            max_length=50,\n",
        "            temperature=0.85\n",
        "        )\n",
        "\n",
        "        print(\"\\n Results \")\n",
        "        for i, text in enumerate(candidates, 1):\n",
        "            print(f\"\\n[{i}] {text}\")\n",
        "\n",
        "\n",
        "interactive_generate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PjxxqTKLb-QD"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}